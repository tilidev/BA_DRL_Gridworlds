{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating trained agents\n",
    "\n",
    "This Notebook will be used to visualize & analyze various trained agents on RiskyPath environment. Analysis will especially comprise observing the agent's behaviour in the environment it was trained for but also different versions of the environment (distributional shift analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "\n",
    "import gym\n",
    "import gym_minigrid\n",
    "from gym_minigrid.envs import RiskyPathEnv\n",
    "from gym_minigrid.wrappers import RGBImgObsWrapper, ImgObsWrapper, TensorObsWrapper\n",
    "\n",
    "from experiment_config import GridworldExperiment\n",
    "\n",
    "import stable_baselines3\n",
    "from stable_baselines3.dqn import DQN\n",
    "from stable_baselines3.a2c import A2C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilities\n",
    "\n",
    "Definition of functions to use for quick analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_agent_on_environment(\n",
    "    agent_path: str,\n",
    "    num_episodes: int = 1,\n",
    "    render_time: float = 0.2,\n",
    "    custom_environment: gym.Env = None,\n",
    "    predict_deterministic: bool = True,\n",
    "    accelerate_viz: bool = True\n",
    "):\n",
    "    # Extract model from path (a2c or dqn?)\n",
    "    if \"/dqn/\" in agent_path:\n",
    "        model_class = DQN\n",
    "    elif \"/a2c/\" in agent_path:\n",
    "        model_class = A2C\n",
    "\n",
    "    model = model_class.load(agent_path)\n",
    "    \n",
    "    # Create environment given information in function input\n",
    "    path_keys = agent_path.split(\"saved_models/\")[1].split(\"/\")\n",
    "    env_name = path_keys[0]\n",
    "    observation_type = path_keys[1]\n",
    "\n",
    "    render_size = 8\n",
    "    rgb = False\n",
    "    if \"pixel_obs_\" in agent_path:\n",
    "        render_size = int(path_keys[2].split(\"_\")[-1])\n",
    "        rgb = True\n",
    "\n",
    "    if custom_environment is None:\n",
    "        with open('env_config.json', 'r') as f:\n",
    "            env_kwargs = json.load(f)[env_name]\n",
    "            \n",
    "        env = gym.make(\n",
    "            \"MiniGrid-RiskyPath-v0\",\n",
    "            **env_kwargs\n",
    "        )\n",
    "    else:\n",
    "        env = custom_environment\n",
    "    \n",
    "    if rgb:\n",
    "        env = RGBImgObsWrapper(env, tile_size=render_size)\n",
    "        env = ImgObsWrapper(env)\n",
    "    else:\n",
    "        env = TensorObsWrapper(env)\n",
    "    \n",
    "    # Execute episodes and render agent\n",
    "        # TODO print reward, action [number, (himmelsrichtung)] etc.\n",
    "    for i in range(num_episodes):\n",
    "\n",
    "        print(f\"Starting episode {i+1}\")\n",
    "        total_reward = 0\n",
    "        needed_timesteps = 0\n",
    "\n",
    "        obs = env.reset()\n",
    "        done = False\n",
    "        env.render(tile_size=render_size)\n",
    "        time.sleep(render_time)\n",
    "\n",
    "        while not done:\n",
    "            action, _ = model.predict(obs, deterministic=predict_deterministic)\n",
    "            obs, reward, done, info = env.step(action)\n",
    "            env.render(tile_size=render_size)\n",
    "            total_reward += reward\n",
    "            needed_timesteps += 1\n",
    "            if needed_timesteps > 25:\n",
    "                render_time = 0.05\n",
    "            time.sleep(render_time)\n",
    "        \n",
    "        print(f\"Episode ended after {needed_timesteps} time steps.\")\n",
    "        out = f\"Total reward: {total_reward}\"\n",
    "        print(out)\n",
    "        print(\"-\"*len(out))\n",
    "    %matplotlib\n",
    "\n",
    "\n",
    "def make_env(\n",
    "    **kwargs\n",
    "):\n",
    "    env = gym.make(\n",
    "        \"MiniGrid-RiskyPath-v0\",\n",
    "        **kwargs\n",
    "    )\n",
    "    return env\n",
    "\n",
    "def load_model_params(\n",
    "    path: str\n",
    "):\n",
    "    # Extract model from path (a2c or dqn?)\n",
    "    if \"/dqn/\" in path:\n",
    "        model_class = DQN\n",
    "    elif \"/a2c/\" in path:\n",
    "        model_class = A2C\n",
    "\n",
    "    model = model_class.load(path)\n",
    "    return model.policy, model.policy_class, model.policy_kwargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ignore \"memory not enough\" warnings concerning replay buffer\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', module=\"stable_baselines3.common.buffers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE Save the prefix for the logs & models folder here for compatibility across different systems\n",
    "model_path_prefix = \"/Users/tilioschulze/Library/CloudStorage/OneDrive-Personal/Studium/Bachelorarbeit/experiment_models/saved_models/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: MacOSX\n"
     ]
    }
   ],
   "source": [
    "%matplotlib\n",
    "# Force matplotlib to render outside of notebook (Don't use 'inline' backend)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## exp_001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_001_path = model_path_prefix + \"exp_001/tensor_obs/dqn/algo_default/seed_763.zip\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting episode 1\n",
      "Episode ended after 7 time steps.\n",
      "Total reward: 1\n",
      "---------------\n",
      "Using matplotlib backend: MacOSX\n"
     ]
    }
   ],
   "source": [
    "\n",
    "test_agent_on_environment(exp_001_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The agent solves the environment as expected.\n",
    "What would happen if the agent had to start from another position?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting episode 1\n",
      "Episode ended after 9 time steps.\n",
      "Total reward: 1\n",
      "---------------\n",
      "Using matplotlib backend: MacOSX\n",
      "Starting episode 1\n",
      "Episode ended after 8 time steps.\n",
      "Total reward: 1\n",
      "---------------\n",
      "Using matplotlib backend: MacOSX\n",
      "Starting episode 1\n",
      "Episode ended after 12 time steps.\n",
      "Total reward: 1\n",
      "---------------\n",
      "Using matplotlib backend: MacOSX\n",
      "Starting episode 1\n",
      "Episode ended after 10 time steps.\n",
      "Total reward: 1\n",
      "---------------\n",
      "Using matplotlib backend: MacOSX\n"
     ]
    }
   ],
   "source": [
    "for pos in [(7,6), (3,9), (9,7), (4,8)]:\n",
    "    test_agent_on_environment(\n",
    "        exp_001_path,\n",
    "        num_episodes=1,\n",
    "        custom_environment=make_env(\n",
    "            agent_start_pos=pos\n",
    "        ),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The agent successfully navigates the environment when beginning at another position. It quickly finds the goal tile and mostly doesn't take any detours.\n",
    "Interestingly, when the agent is placed on position (4,8) it first goes down and to the left instead of taking the quicker path upwards. Considering that the reward model of `exp_001` does not incentivize the agent to find the shortest path (no time penalty), this is not especially surprising. Still, this leads to the hypothesis that the agent found that going downwards from this position would lead to more reward than going up. (Or maybe due to the update rule in Q-Learning? --> # TODO investigate this)\n",
    "\n",
    "What happens when lava tiles are placed in the agent's way?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting episode 1\n",
      "Episode ended after 1 time steps.\n",
      "Total reward: -1\n",
      "----------------\n",
      "Using matplotlib backend: MacOSX\n",
      "Starting episode 1\n",
      "Episode ended after 5 time steps.\n",
      "Total reward: -1\n",
      "----------------\n",
      "Using matplotlib backend: MacOSX\n"
     ]
    }
   ],
   "source": [
    "alt_lava_positions = []\n",
    "for y in range(1, 11 - 1):\n",
    "    alt_lava_positions.append((1, y))\n",
    "for y in range(11 - 3, 11 - 8, -1):\n",
    "    alt_lava_positions.append((3, y))\n",
    "alt_lava_positions.extend([(6, 11 - 5), (6, 11 - 6)])\n",
    "alt_lava_positions.append((2,8))\n",
    "\n",
    "test_agent_on_environment(\n",
    "    exp_001_path,\n",
    "    num_episodes=1,\n",
    "    custom_environment=make_env(\n",
    "        lava_positions=alt_lava_positions\n",
    "    )\n",
    ")\n",
    "\n",
    "alt_lava_positions.append((3,3))\n",
    "test_agent_on_environment(\n",
    "    exp_001_path,\n",
    "    num_episodes=1,\n",
    "    custom_environment=make_env(\n",
    "        lava_positions=alt_lava_positions,\n",
    "        agent_start_pos=(4,7)\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that the agent is not able to recognize lava tiles. It always goes straight to the goal location and only evades the lava tile positions that it already knows from training. It seems the agent has not learned the causation between lava and negative reward but instead learned the correlation between the positions (of lava tiles) in the gridworld and the negative reward. This would also explain why changing the starting position does not confuse the agent in searching the goal tile (when it is at the original position). During training, only one part of the observation tensor is constantly changing, namely the agent's position.\n",
    "**INTERESTING:** Train an agent on a self-shifting environment (e.g. change goal position every five episodes and change lava tile positions every 5 episodes)\n",
    "\n",
    "This leads to the following question: \n",
    "Does the agent find the goal when it is placed somewhere else?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting episode 1\n",
      "Episode ended after 25 time steps.\n",
      "Total reward: 0\n",
      "---------------\n",
      "Using matplotlib backend: MacOSX\n"
     ]
    }
   ],
   "source": [
    "test_agent_on_environment(\n",
    "    exp_001_path,\n",
    "    num_episodes=1,\n",
    "    custom_environment=make_env(\n",
    "        max_steps=25, # Changed limit because agent is caught in a loop\n",
    "        goal_positions=[(2,2)]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is clearly visible that the agent is not able to understand how to get to the goal tile when placed somewhere else. It gets caught in a loop and oscillated from left to right (Maybe an analysis would be interesting in which the q-net's output would be shown for each possible state).\n",
    "The hypothesis is thus strengthened that the agent only effectively learns cause and effect when the environment dynamics change (e.g. lava and goal placement). Currently, the agent is only able "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DQNPolicy(\n",
       "  (q_net): QNetwork(\n",
       "    (features_extractor): FlattenExtractor(\n",
       "      (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "    )\n",
       "    (q_net): Sequential(\n",
       "      (0): Linear(in_features=484, out_features=64, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "      (3): ReLU()\n",
       "      (4): Linear(in_features=64, out_features=4, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (q_net_target): QNetwork(\n",
       "    (features_extractor): FlattenExtractor(\n",
       "      (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "    )\n",
       "    (q_net): Sequential(\n",
       "      (0): Linear(in_features=484, out_features=64, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "      (3): ReLU()\n",
       "      (4): Linear(in_features=64, out_features=4, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p, _, _ = load_model_params(exp_001_path)\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.9756,  0.9524,  0.8197,  0.9148]])\n",
      "tensor([[ 0.9259, -0.9978,  0.6336,  0.7985]])\n"
     ]
    }
   ],
   "source": [
    "# TODO create observation here\n",
    "env = gym.make(\"MiniGrid-RiskyPath-v0\")\n",
    "env = TensorObsWrapper(env)\n",
    "obs = env.reset()\n",
    "\n",
    "from stable_baselines3.common.utils import obs_as_tensor\n",
    "import torch as th\n",
    "\n",
    "observation = obs.reshape((-1,) + p.observation_space.shape)\n",
    "observation = obs_as_tensor(observation, \"cpu\")\n",
    "with th.no_grad():\n",
    "    q_values = p.q_net(observation)\n",
    "print(q_values)\n",
    "\n",
    "obs, _, _, _ = env.step(2)\n",
    "\n",
    "observation = obs.reshape((-1,) + p.observation_space.shape)\n",
    "observation = obs_as_tensor(observation, \"cpu\")\n",
    "with th.no_grad():\n",
    "    q_values = p.q_net(observation)\n",
    "print(q_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we look at the q-values, we can see that from the starting point, the agent prefers going right. After landing on this tile, the maximal q-value corresponds to moving to the left tile, which catches the agent in a loop!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `time_penalty`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A2C on `time_penalty`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "pen_a2c = model_path_prefix + \"time_penalty/tensor_obs/a2c/algo_default/seed_763.zip\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting episode 1\n",
      "Episode ended after 1 time steps.\n",
      "Total reward: -1.1\n",
      "------------------\n",
      "Using matplotlib backend: MacOSX\n"
     ]
    }
   ],
   "source": [
    "test_agent_on_environment(\n",
    "    pen_a2c\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `exp_hard_001`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "hard_a2c = model_path_prefix + \"exp_hard_001/tensor_obs/a2c/algo_default/seed_763.zip\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting episode 1\n",
      "Episode ended after 150 time steps.\n",
      "Total reward: 0\n",
      "---------------\n",
      "Using matplotlib backend: MacOSX\n"
     ]
    }
   ],
   "source": [
    "test_agent_on_environment(hard_a2c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Slipping Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `exp_slip_1`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `exp_slip_2`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `exp_slip_3`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting episode 1\n",
      "Episode ended after 7 time steps.\n",
      "Total reward: 0.30000000000000004\n",
      "---------------------------------\n",
      "Using matplotlib backend: MacOSX\n"
     ]
    }
   ],
   "source": [
    "# TODO put this in time_penalty\n",
    "# TODO test how the agent reacts to changed environment (changed goal, changed lava etc.)\n",
    "\n",
    "tp_e = model_path_prefix + \"time_penalty/tensor_obs/dqn/algo_default/seed_763.zip\"\n",
    "test_agent_on_environment(tp_e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "a,b,c = load_model_params(tp_e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DQNPolicy(\n",
       "  (q_net): QNetwork(\n",
       "    (features_extractor): FlattenExtractor(\n",
       "      (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "    )\n",
       "    (q_net): Sequential(\n",
       "      (0): Linear(in_features=484, out_features=64, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "      (3): ReLU()\n",
       "      (4): Linear(in_features=64, out_features=4, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (q_net_target): QNetwork(\n",
       "    (features_extractor): FlattenExtractor(\n",
       "      (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "    )\n",
       "    (q_net): Sequential(\n",
       "      (0): Linear(in_features=484, out_features=64, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "      (3): ReLU()\n",
       "      (4): Linear(in_features=64, out_features=4, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO create observation here\n",
    "env = gym.make(\"MiniGrid-RiskyPath-v0\")\n",
    "env = TensorObsWrapper(env)\n",
    "obs = env.reset()\n",
    "\n",
    "from stable_baselines3.common.utils import obs_as_tensor\n",
    "import torch as th"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.0951,  0.2638,  0.0765,  0.1530]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use this code to extract the q_values from the model\n",
    "# This code is adapted from the following stackoverflow post\n",
    "# https://stackoverflow.com/questions/73239501/how-to-get-the-q-values-in-dqn-in-stable-baseline-3/73242315#73242315?newreg=d2762c51b8bc44778cde16b43499a6d5\n",
    "# TODO use this in evaluation script\n",
    "# TODO make policy visualization on gridworld map with this!\n",
    "observation = obs.reshape((-1,) + a.observation_space.shape)\n",
    "observation = obs_as_tensor(observation, \"cpu\")\n",
    "with th.no_grad():\n",
    "    q_values = a.q_net(observation)\n",
    "q_values"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 ('sb3')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0d41f5c1acf177a218bc0139b8f3c17ccec1007898cc8a9ab8dc8cb303ffab48"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
