{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating trained agents\n",
    "\n",
    "This Notebook will be used to visualize & analyze various trained agents on RiskyPath environment. Analysis will especially comprise observing the agent's behaviour in the environment it was trained for but also different versions of the environment (distributional shift analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "\n",
    "import gym\n",
    "import gym_minigrid\n",
    "from gym_minigrid.envs import RiskyPathEnv\n",
    "from gym_minigrid.wrappers import RGBImgObsWrapper, ImgObsWrapper, TensorObsWrapper\n",
    "from special_wrappers import RandomizeGoalWrapper\n",
    "\n",
    "from experiment_config import GridworldExperiment\n",
    "import torch as th\n",
    "import stable_baselines3\n",
    "from stable_baselines3.dqn import DQN\n",
    "from stable_baselines3.a2c import A2C\n",
    "from stable_baselines3.common.utils import obs_as_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilities\n",
    "\n",
    "Definition of functions to use for quick analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_agent_on_environment(\n",
    "    agent_path: str,\n",
    "    num_episodes: int = 1,\n",
    "    render_time: float = 0.2,\n",
    "    custom_environment: gym.Env = None,\n",
    "    predict_deterministic: bool = True,\n",
    "    accelerate_viz: bool = True\n",
    "):\n",
    "    # Extract model from path (a2c or dqn?)\n",
    "    if \"/dqn/\" in agent_path:\n",
    "        model_class = DQN\n",
    "    elif \"/a2c/\" in agent_path:\n",
    "        model_class = A2C\n",
    "\n",
    "    model = model_class.load(agent_path)\n",
    "    \n",
    "    # Create environment given information in function input\n",
    "    path_keys = agent_path.split(\"saved_models/\")[1].split(\"/\")\n",
    "    env_name = path_keys[0]\n",
    "    observation_type = path_keys[1]\n",
    "\n",
    "    render_size = 8\n",
    "    rgb = False\n",
    "    if \"pixel_obs_\" in agent_path:\n",
    "        render_size = int(path_keys[1].split(\"_\")[-1])\n",
    "        rgb = True\n",
    "\n",
    "    if custom_environment is None:\n",
    "        with open('env_config.json', 'r') as f:\n",
    "            env_kwargs = json.load(f)[env_name]\n",
    "        if 'goal_rnd' in env_kwargs:\n",
    "            env_kwargs.pop('goal_rnd')\n",
    "\n",
    "        env = gym.make(\n",
    "            \"MiniGrid-RiskyPath-v0\",\n",
    "            **env_kwargs\n",
    "        )\n",
    "    else:\n",
    "        env = custom_environment\n",
    "    \n",
    "    if rgb:\n",
    "        env = RGBImgObsWrapper(env, tile_size=render_size)\n",
    "        env = ImgObsWrapper(env)\n",
    "    else:\n",
    "        env = TensorObsWrapper(env)\n",
    "    \n",
    "    # Execute episodes and render agent\n",
    "        # TODO print reward, action [number, (himmelsrichtung)] etc.\n",
    "    for i in range(num_episodes):\n",
    "\n",
    "        print(f\"Starting episode {i+1}\")\n",
    "        total_reward = 0\n",
    "        needed_timesteps = 0\n",
    "\n",
    "        obs = env.reset()\n",
    "        done = False\n",
    "        env.render(tile_size=render_size)\n",
    "        time.sleep(render_time)\n",
    "\n",
    "        while not done:\n",
    "            action, _ = model.predict(obs, deterministic=predict_deterministic)\n",
    "            obs, reward, done, info = env.step(action)\n",
    "            env.render(tile_size=render_size)\n",
    "            total_reward += reward\n",
    "            needed_timesteps += 1\n",
    "            if needed_timesteps > 25:\n",
    "                render_time = 0.05\n",
    "            time.sleep(render_time)\n",
    "        \n",
    "        print(f\"Episode ended after {needed_timesteps} time steps.\")\n",
    "        out = f\"Total reward: {total_reward}\"\n",
    "        print(out)\n",
    "        print(\"-\"*len(out))\n",
    "    \n",
    "    %matplotlib\n",
    "\n",
    "\n",
    "def make_env(\n",
    "    **kwargs\n",
    "):\n",
    "    env = gym.make(\n",
    "        \"MiniGrid-RiskyPath-v0\",\n",
    "        **kwargs\n",
    "    )\n",
    "    return env\n",
    "\n",
    "def load_model_params(\n",
    "    path: str\n",
    "):\n",
    "    \"\"\"Return model policy and additional information\n",
    "\n",
    "    Args:\n",
    "        path (str): The path to the saved model\n",
    "\n",
    "    Returns:\n",
    "        tuple: policy, policy_class, policy_kwargs\n",
    "    \"\"\"\n",
    "    # Extract model from path (a2c or dqn?)\n",
    "    if \"/dqn/\" in path:\n",
    "        model_class = DQN\n",
    "    elif \"/a2c/\" in path:\n",
    "        model_class = A2C\n",
    "\n",
    "    model = model_class.load(path)\n",
    "    return model.policy, model.policy_class, model.policy_kwargs\n",
    "\n",
    "def dqn_params(path):\n",
    "    if \"/dqn/\" in path:\n",
    "        model_class = DQN\n",
    "    model = model_class.load(path)\n",
    "    return model.get_parameters()\n",
    "\n",
    "def compute_q_values(model_policy, obs):\n",
    "    \"\"\"Compute q-values from a DQN model given a certain observation.\n",
    "\n",
    "    Args:\n",
    "        model_policy: The DQN model's policy\n",
    "        obs: The environmental observation for which q-values should be computed\n",
    "    \"\"\"\n",
    "    # Code adapted from this stackoverflow post\n",
    "    # https://stackoverflow.com/questions/73239501/how-to-get-the-q-values-in-dqn-in-stable-baseline-3/73242315#73242315?newreg=d2762c51b8bc44778cde16b43499a6d5\n",
    "    observation = obs.reshape((-1,) + model_policy.observation_space.shape)\n",
    "    observation = obs_as_tensor(observation, \"cpu\")\n",
    "    with th.no_grad():\n",
    "        q_values = model_policy.q_net(observation)\n",
    "    return q_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ignore \"memory not enough\" warnings concerning replay buffer\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', module=\"stable_baselines3.common.buffers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE Save the prefix for the logs & models folder here for compatibility across different systems\n",
    "model_path_prefix = \"/Users/tilioschulze/Library/CloudStorage/OneDrive-Personal/Studium/Bachelorarbeit/experiment_models/saved_models/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: <object object at 0x1574c8e40>\n"
     ]
    }
   ],
   "source": [
    "%matplotlib\n",
    "# Force matplotlib to render outside of notebook (Don't use 'inline' backend)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## exp_001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's load one of the successful trained agents on the `stable-baselines3` DQN defaults."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_001_path = model_path_prefix + \"exp_001/tensor_obs/dqn/algo_default/seed_763.zip\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_agent_on_environment(exp_001_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The agent solves the environment as expected.\n",
    "What would happen if the agent had to start from another position?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pos in [(7,6), (3,9), (9,7), (4,8)]:\n",
    "    test_agent_on_environment(\n",
    "        exp_001_path,\n",
    "        num_episodes=1,\n",
    "        custom_environment=make_env(\n",
    "            agent_start_pos=pos\n",
    "        ),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The agent successfully navigates the environment when beginning at another position. It quickly finds the goal tile and mostly doesn't take any detours.\n",
    "Interestingly, when the agent is placed on position (4,8) it first goes down and to the left instead of taking the quicker path upwards. Considering that the reward model of `exp_001` does not incentivize the agent to find the shortest path (no time penalty), this is not especially surprising. Still, this leads to the hypothesis that the agent found that going downwards from this position would lead to more reward than going up. (Or maybe due to the update rule in Q-Learning? --> # TODO investigate this)\n",
    "\n",
    "What happens when lava tiles are placed in the agent's way?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt_lava_positions = []\n",
    "for y in range(1, 11 - 1):\n",
    "    alt_lava_positions.append((1, y))\n",
    "for y in range(11 - 3, 11 - 8, -1):\n",
    "    alt_lava_positions.append((3, y))\n",
    "alt_lava_positions.extend([(6, 11 - 5), (6, 11 - 6)])\n",
    "alt_lava_positions.append((2,8))\n",
    "\n",
    "test_agent_on_environment(\n",
    "    exp_001_path,\n",
    "    num_episodes=1,\n",
    "    custom_environment=make_env(\n",
    "        lava_positions=alt_lava_positions\n",
    "    )\n",
    ")\n",
    "\n",
    "alt_lava_positions.append((3,3))\n",
    "test_agent_on_environment(\n",
    "    exp_001_path,\n",
    "    num_episodes=1,\n",
    "    custom_environment=make_env(\n",
    "        lava_positions=alt_lava_positions,\n",
    "        agent_start_pos=(4,7)\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that the agent is not able to recognize lava tiles. It always goes straight to the goal location and only evades the lava tile positions that it already knows from training. It seems the agent has not learned the causation between lava and negative reward but instead learned the correlation between the positions (of lava tiles) in the gridworld and the negative reward. This would also explain why changing the starting position does not confuse the agent in searching the goal tile (when it is at the original position). During training, only one part of the observation tensor is constantly changing, namely the agent's position.\n",
    "**INTERESTING:** Train an agent on a self-shifting environment (e.g. change goal position every five episodes and change lava tile positions every 5 episodes)\n",
    "\n",
    "This leads to the following question: \n",
    "Does the agent find the goal when it is placed somewhere else?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_agent_on_environment(\n",
    "    exp_001_path,\n",
    "    num_episodes=1,\n",
    "    custom_environment=make_env(\n",
    "        max_steps=25, # Changed limit because agent is caught in a loop\n",
    "        goal_positions=[(2,2)]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is clearly visible that the agent is not able to understand how to get to the goal tile when placed somewhere else. It gets caught in a loop and oscillated from left to right (Maybe an analysis would be interesting in which the q-net's output would be shown for each possible state).\n",
    "The hypothesis is thus strengthened that the agent only effectively learns cause and effect when the environment dynamics change (e.g. lava and goal placement). Currently, the agent is only able "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load an a2c model that was very succesful during training. The next observation is an interesting one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_lava = []\n",
    "for y in range(1, 11 - 1):\n",
    "    orig_lava.append((1, y))\n",
    "for y in range(11 - 3, 11 - 8, -1):\n",
    "    orig_lava.append((3, y))\n",
    "orig_lava.remove((1,3))\n",
    "\n",
    "a2c_low_entropy_model = model_path_prefix + \"exp_001/tensor_obs/a2c/a2c_entropy_6/seed_4267.zip\"\n",
    "\n",
    "test_agent_on_environment(\n",
    "    a2c_low_entropy_model,\n",
    "    num_episodes=1,\n",
    "    custom_environment=make_env(\n",
    "        max_steps=25,\n",
    "        goal_positions=[(3,3)],\n",
    "        lava_positions=orig_lava\n",
    "    ),\n",
    "    render_time=0.4\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Distributional Shift, Goal Misgeneralization:**\n",
    "\n",
    "The agent does not understand that the important tile is the goal tile. It still navigates to the position in which it recieved positive rewards during training. Once the state distribution shifts, the agent is not able to apply the learned skills to a simple alteration of the environment. This shows that the state representation during training is not truly sufficient if we want the agent to be able to generalize knowledge.\n",
    "\n",
    "- [ ] Train agent on a self-shifting environment with different goal positions. Use algorithmic settings/parametrizations that worked best during training\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test on self-shifting environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_agent_on_environment(\n",
    "    a2c_low_entropy_model,\n",
    "    num_episodes=5,\n",
    "    render_time=0.4,\n",
    "    custom_environment=RandomizeGoalWrapper(make_env(), randomization=0.5)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The agent only reaches the goal tile when it is placed in on the training location! The agent has not learned to walk in the goal tile, it has learned to walk to the location where it recieved rewards during training. During training, this is a **perfect proxy of the intended goal**, which is to walk on the goal tile! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Goal Randomization Agent:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO train on cluster and then remove\n",
    "best_exp1_rnd_agent = \"/Users/tilioschulze/Desktop/Bachelorarbeit/Code/Experiments/saved_models/exp_001_goal_rnd_2/tensor_obs/dqn/algo_default/seed_763_best_model/best_model.zip\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting episode 1\n",
      "Episode ended after 7 time steps.\n",
      "Total reward: 1\n",
      "---------------\n",
      "Using matplotlib backend: MacOSX\n"
     ]
    }
   ],
   "source": [
    "# TODO train on cluster and then remove\n",
    "test_agent_on_environment(\n",
    "    best_exp1_rnd_agent\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting episode 1\n",
      "Episode ended after 150 time steps.\n",
      "Total reward: 0\n",
      "---------------\n",
      "Using matplotlib backend: MacOSX\n"
     ]
    }
   ],
   "source": [
    "# TODO train on cluster and then remove\n",
    "test_agent_on_environment(\n",
    "    best_exp1_rnd_agent,\n",
    "    custom_environment=make_env(\n",
    "        goal_positions=[(9,9)]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agent is not yet able to find its way. At least it does not walk straight in the lava tile.\n",
    "It's just not yet confident enugh to find the goal\n",
    "- [ ] remove this cell too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p, _, _ = load_model_params(exp_001_path)\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO create observation here\n",
    "env = gym.make(\"MiniGrid-RiskyPath-v0\")\n",
    "env = TensorObsWrapper(env)\n",
    "obs = env.reset()\n",
    "\n",
    "from stable_baselines3.common.utils import obs_as_tensor\n",
    "import torch as th\n",
    "\n",
    "observation = obs.reshape((-1,) + p.observation_space.shape)\n",
    "observation = obs_as_tensor(observation, \"cpu\")\n",
    "with th.no_grad():\n",
    "    q_values = p.q_net(observation)\n",
    "print(q_values)\n",
    "\n",
    "obs, _, _, _ = env.step(2)\n",
    "\n",
    "observation = obs.reshape((-1,) + p.observation_space.shape)\n",
    "observation = obs_as_tensor(observation, \"cpu\")\n",
    "with th.no_grad():\n",
    "    q_values = p.q_net(observation)\n",
    "print(q_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we look at the q-values, we can see that from the starting point, the agent prefers going right. After landing on this tile, the maximal q-value corresponds to moving to the left tile, which catches the agent in a loop!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A2C low entropy model on exp_001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "npath = model_path_prefix + \"exp_001/pixel_obs_8/a2c/a2c_entropy_6/seed_3377.zip\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_agent_on_environment(\n",
    "    npath\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This training-wise succesful a2c agent is also not able to generalize, when confronted with new lava tile positions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A2C entropy_6 on exp_001 (pixel_obs_8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_agent_on_environment(\n",
    "    model_path_prefix + \"exp_hard_001/pixel_obs_8/a2c/a2c_entropy_6/seed_4267.zip\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Success! The agent learns to walk around the spiky tiles and lava to maximize rewards.\n",
    "However, this is only one seed out of 5. The other 4 seeds failed to find such a solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt_lava_positions = []\n",
    "for y in range(1, 11 - 1):\n",
    "    alt_lava_positions.append((1, y))\n",
    "for y in range(11 - 3, 11 - 8, -1):\n",
    "    alt_lava_positions.append((3, y))\n",
    "alt_lava_positions.extend([(6, 11 - 5), (6, 11 - 6)])\n",
    "\n",
    "spiky_positions = []\n",
    "\n",
    "test_agent_on_environment(\n",
    "    model_path_prefix + \"exp_hard_001/pixel_obs_8/a2c/a2c_entropy_6/seed_4267.zip\",\n",
    "    custom_environment=make_env(\n",
    "        lava_positions=alt_lava_positions\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting --> Removing spiky tiles confuses the agent at the last step. Maybe it expects to walk on a spiky tile when finishing the Umweg, but does not recognize the situation.\n",
    "What happens when moving the goal tile?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_agent_on_environment(\n",
    "    model_path_prefix + \"exp_hard_001/pixel_obs_8/a2c/a2c_entropy_6/seed_4744.zip\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `time_penalty`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A2C on `time_penalty`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pen_a2c = model_path_prefix + \"time_penalty/tensor_obs/a2c/algo_default/seed_763.zip\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_agent_on_environment(\n",
    "    pen_a2c\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `exp_hard_001`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hard_a2c = model_path_prefix + \"exp_hard_001/tensor_obs/a2c/algo_default/seed_763.zip\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_agent_on_environment(hard_a2c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Slipping Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `exp_slip_1`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `exp_slip_2`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `exp_slip_3`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO put this in time_penalty\n",
    "# TODO test how the agent reacts to changed environment (changed goal, changed lava etc.)\n",
    "\n",
    "tp_e = model_path_prefix + \"time_penalty/tensor_obs/dqn/algo_default/seed_763.zip\"\n",
    "test_agent_on_environment(tp_e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a,b,c = load_model_params(tp_e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## exp_hard_001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A2C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a2c_def = model_path_prefix + \"exp_hard_001/tensor_obs/a2c/algo_default/seed_763.zip\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_agent_on_environment(a2c_def)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 ('sb3')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0d41f5c1acf177a218bc0139b8f3c17ccec1007898cc8a9ab8dc8cb303ffab48"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
