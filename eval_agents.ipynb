{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating trained agents\n",
    "\n",
    "This Notebook will be used to visualize & analyze various trained agents on RiskyPath environment. Analysis will especially comprise observing the agent's behaviour in the environment it was trained for but also different versions of the environment (distributional shift analysis)\n",
    "\n",
    "To observe deterministic agent behaviour, slipping/collision probabilities will be automatically set to zero. In this case, the agent might not be tested on the exact same environment configuration it was trained on. However, other environmental factors will behave as per the training environment's configuration. This is not the case if the model is explicitly tested on custom environments, which is obvious in the corresponding cells.\n",
    "\n",
    "The next cell defines the folder location prefix for saved models. Set this to the location in which you have downloaded the trained models.\n",
    "**Warning**: The folder structure of the saved models (inside `saved_models/â€¦`) must not be changed and stay in the format defined by `experiment_config.py`. This is necessary as code in this notebook uses the path's information to infer environment configuration, model specifics etc.\n",
    "\n",
    "Some cells are saved as raw format. They contain code which mostly renders agent behaviour in a specific environmental setting. This is especially useful for making videos and watching agent interaction, but otherwise not needed. \n",
    "\n",
    "In Jupyter, one can change raw or markdown cells to code cells by entering the command mode in the cell by pressing `esc` and then `y`. To return to raw format, use `esc` and `r`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE Save the prefix for the models folder here for compatibility across different systems\n",
    "model_path_prefix = \"/Users/tilioschulze/Library/CloudStorage/OneDrive-Personal/Studium/Bachelorarbeit/experiment_models/saved_models/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "\n",
    "import gym\n",
    "import gym_minigrid\n",
    "from gym_minigrid.minigrid import Goal, Floor, Lava, Wall, SpikyTile\n",
    "from gym_minigrid.envs import RiskyPathEnv\n",
    "from gym_minigrid.wrappers import RGBImgObsWrapper, ImgObsWrapper, TensorObsWrapper\n",
    "from special_wrappers import RandomizeGoalWrapper\n",
    "\n",
    "from experiment_config import GridworldExperiment\n",
    "import torch as th\n",
    "import stable_baselines3\n",
    "from stable_baselines3.dqn import DQN\n",
    "from stable_baselines3.a2c import A2C\n",
    "from stable_baselines3.common.utils import obs_as_tensor\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilities\n",
    "\n",
    "Definition of functions to use for quick analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sinfo = \"\\33[32mINFO:\\33[0m\"\n",
    "\n",
    "def model_env_from_path(agent_path: str, no_slip: bool = True, no_rebound: bool = True):\n",
    "    \"\"\"Extract model, environment, observation type and tile render size from information in the model's save-path.\n",
    "    In order to allow deterministic analysis of agent-environment interaction, slipping and wall rebound are turned off per default.\n",
    "    \"\"\"    \n",
    "    # Extract model from path (a2c or dqn?)\n",
    "    if \"/dqn/\" in agent_path:\n",
    "        model_class = DQN\n",
    "    elif \"/a2c/\" in agent_path:\n",
    "        model_class = A2C\n",
    "\n",
    "    model = model_class.load(agent_path)\n",
    "\n",
    "    # Create environment given information in function input\n",
    "    path_keys = agent_path.split(\"saved_models/\")[1].split(\"/\")\n",
    "    env_name = path_keys[0]\n",
    "    observation_type = path_keys[1]\n",
    "\n",
    "    render_size = 8\n",
    "    rgb = False\n",
    "    if \"pixel_obs_\" in agent_path:\n",
    "        render_size = int(path_keys[1].split(\"_\")[-1])\n",
    "        rgb = True\n",
    "\n",
    "    env_info = \"\"\n",
    "    with open('env_config.json', 'r') as f:\n",
    "        env_kwargs = json.load(f)[env_name]\n",
    "    if 'goal_rnd' in env_kwargs:\n",
    "        env_kwargs.pop('goal_rnd')\n",
    "    if no_slip and env_kwargs['slip_proba'] != 0:\n",
    "        env_kwargs.pop('slip_proba')\n",
    "        env_info += \"slipping probability removed; \"\n",
    "    if no_rebound and env_kwargs['wall_rebound']:\n",
    "        env_kwargs.pop('wall_rebound')\n",
    "        env_info += \"wall rebound deactivated\"\n",
    "    if len(env_info) != 0:\n",
    "        print(\"\\33[32mINFO:\\33[0m\", env_info)\n",
    "\n",
    "    env = gym.make(\n",
    "        \"MiniGrid-RiskyPath-v0\",\n",
    "        **env_kwargs\n",
    "    )\n",
    "\n",
    "    return model, env, rgb, render_size\n",
    "\n",
    "def test_agent_on_environment(\n",
    "    agent_path: str,\n",
    "    num_episodes: int = 1,\n",
    "    render_time: float = 0.2,\n",
    "    custom_environment: gym.Env = None,\n",
    "    predict_deterministic: bool = True,\n",
    "    accelerate_viz: bool = True\n",
    "):\n",
    "    \"\"\"Render agent interaction with the environment in an interactive matplotlib window. Useful to make videos of agent behaviour or analyzing trajectories. Slipping and wall rebound is turned off in order to observe the agent's intended behaviour. When passing a custom environment, no checking for stationary state distribution and deterministic transitions is performed.\n",
    "\n",
    "    Args:\n",
    "        agent_path (str): Model location. Folder path must conform to experiment structure\n",
    "        num_episodes (int, optional): number of episodes to render\n",
    "        render_time (float, optional): render time for one time step in seconds\n",
    "        custom_environment (gym.Env, optional): None by default\n",
    "        predict_deterministic (bool, optional): Make deterministic mode predictions\n",
    "        accelerate_viz (bool, optional): Will accelerate rendering when agent takes too long to solve environment\n",
    "    \"\"\"\n",
    "    model, env, rgb_on, render_size = model_env_from_path(agent_path)\n",
    "\n",
    "    if custom_environment is not None:\n",
    "        env = custom_environment\n",
    "    \n",
    "    if rgb_on:\n",
    "        env = RGBImgObsWrapper(env, tile_size=render_size)\n",
    "        env = ImgObsWrapper(env)\n",
    "    else:\n",
    "        env = TensorObsWrapper(env)\n",
    "    \n",
    "    # Execute episodes and render agent\n",
    "        # TODO print reward, action [number, (himmelsrichtung)] etc.\n",
    "    for i in range(num_episodes):\n",
    "\n",
    "        print(f\"Starting episode {i+1}\")\n",
    "        total_reward = 0\n",
    "        needed_timesteps = 0\n",
    "\n",
    "        obs = env.reset()\n",
    "        done = False\n",
    "        env.render(tile_size=render_size)\n",
    "        time.sleep(render_time)\n",
    "\n",
    "        while not done:\n",
    "            action, _ = model.predict(obs, deterministic=predict_deterministic)\n",
    "            obs, reward, done, info = env.step(action)\n",
    "            env.render(tile_size=render_size)\n",
    "            total_reward += reward\n",
    "            needed_timesteps += 1\n",
    "            if needed_timesteps > 25:\n",
    "                render_time = 0.05\n",
    "            time.sleep(render_time)\n",
    "        \n",
    "        print(f\"Episode ended after {needed_timesteps} time steps.\")\n",
    "        out = f\"Total reward: {total_reward}\"\n",
    "        print(out)\n",
    "        print(\"-\"*len(out))\n",
    "    \n",
    "    %matplotlib\n",
    "\n",
    "def make_env(\n",
    "    **kwargs\n",
    "):\n",
    "    env = gym.make(\n",
    "        \"MiniGrid-RiskyPath-v0\",\n",
    "        **kwargs\n",
    "    )\n",
    "    return env\n",
    "\n",
    "def compute_q_values(model_policy, obs):\n",
    "    \"\"\"Compute q-values from a DQN model given a certain observation.\n",
    "\n",
    "    Args:\n",
    "        model_policy: The DQN model's policy\n",
    "        obs: The environmental observation for which q-values should be computed\n",
    "    \"\"\"\n",
    "    # Code adapted from this stackoverflow post\n",
    "    # https://stackoverflow.com/questions/73239501/how-to-get-the-q-values-in-dqn-in-stable-baseline-3/73242315#73242315?newreg=d2762c51b8bc44778cde16b43499a6d5\n",
    "    observation = obs.reshape((-1,) + model_policy.observation_space.shape)\n",
    "    observation = obs_as_tensor(observation, \"cpu\")\n",
    "    with th.no_grad():\n",
    "        q_values = model_policy.q_net(observation)\n",
    "    return q_values\n",
    "\n",
    "def visualize_policy(\n",
    "    path: str,\n",
    "    custom_env = None\n",
    "):\n",
    "    \"\"\"Visualize the model policy on the given environment specification. The environmental state distribution is assumed to stationary. Goal randomization is explicitly not applied. A custom environment can be passed and policy will be applied on it. This method does not check if the state distribution is stationary. Returns a colored string representation to print to the console.\n",
    "\n",
    "    Args:\n",
    "        path (str): The saved models location\n",
    "        custom_env (RiskyPathEnv): A custom environment\n",
    "    \"\"\"    \n",
    "    model, env, rgb_on, render_size = model_env_from_path(path)\n",
    "    if custom_env is not None:\n",
    "        env = custom_env\n",
    "    \n",
    "    # No wrapping is needed\n",
    "    env.reset()\n",
    "\n",
    "    grid = env.grid\n",
    "    visual_policy = \"\"\n",
    "\n",
    "    ansi_color = lambda code, text:  f\"\\33[{code}m{text}\\33[0m\"\n",
    "\n",
    "    for i in range(grid.width):\n",
    "        visual_policy += \" \" + str(i) + \"  \"\n",
    "        if i == grid.width - 1:\n",
    "            visual_policy += \"\\n\"\n",
    "\n",
    "    for y in range(grid.height):\n",
    "        for x in range(grid.width):\n",
    "            tile = grid.get(x, y)\n",
    "            \n",
    "            if tile is None or isinstance(tile, Floor) or isinstance(tile, SpikyTile):\n",
    "                # get model action and map to <, >, ^, v strings\n",
    "                # NOTE setting a variable only works on unwrapped env as gym automatically wraps the environment with orderenforcing wrapper and wrappers do not implement a __setattr__ method but a __getattr__\n",
    "                env.unwrapped.agent_pos = (x, y)\n",
    "                if rgb_on:\n",
    "                    obs = env.render(\n",
    "                        mode=\"rgb_array\",\n",
    "                        highlight=False,\n",
    "                        tile_size=render_size\n",
    "                    )\n",
    "                else:\n",
    "                    obs = env.tensor_obs()\n",
    "\n",
    "                dir_mapping = {0 : \"<\", 1 : \"^\", 2 : \">\", 3 : \"v\"}\n",
    "                action = int(model.predict(obs, deterministic=True)[0])\n",
    "                dir_str = dir_mapping[action]\n",
    "\n",
    "                visual_policy += f\"[{dir_str}] \"\n",
    "            elif isinstance(tile, Wall):\n",
    "                w = ansi_color(36, \"#\")\n",
    "                visual_policy += f\"[{w}] \"\n",
    "            elif isinstance(tile, Lava):\n",
    "                l = ansi_color(41, \"~\")\n",
    "                visual_policy += f\"[{l}] \"\n",
    "            elif isinstance(tile, Goal):\n",
    "                g = ansi_color(42, \"x\")\n",
    "                visual_policy += f\"[{g}] \"\n",
    "            \n",
    "            if x == grid.width - 1: \n",
    "                visual_policy += f\" {y} \\n\"\n",
    "            \n",
    "    return visual_policy\n",
    "\n",
    "def load_model_params(\n",
    "    path: str\n",
    "):\n",
    "    \"\"\"Return model policy and additional information\n",
    "\n",
    "    Args:\n",
    "        path (str): The path to the saved model\n",
    "\n",
    "    Returns:\n",
    "        tuple: policy, policy_class, policy_kwargs\n",
    "    \"\"\"\n",
    "    # Extract model from path (a2c or dqn?)\n",
    "    if \"/dqn/\" in path:\n",
    "        model_class = DQN\n",
    "    elif \"/a2c/\" in path:\n",
    "        model_class = A2C\n",
    "\n",
    "    model = model_class.load(path)\n",
    "    return model.policy, model.policy_class, model.policy_kwargs\n",
    "\n",
    "def dqn_params(path):\n",
    "    if \"/dqn/\" in path:\n",
    "        model_class = DQN\n",
    "    model = model_class.load(path)\n",
    "    return model.get_parameters()\n",
    "\n",
    "def randomized_goal_stats(path: str, episodes: int = 50):\n",
    "    \"\"\"Test the agent statistically on the training environment specification but with randomized goal tile placement. Other environmental factors are taken from the env_config key except for slipping and wall rebound which is turned off in order to truly analyze the agent's capabilities.\n",
    "\n",
    "    Args:\n",
    "        path (str): Location of the saved model. Must conform to predefined folder structure.\n",
    "        episodes (int, optional): Number random-goal episodes\n",
    "    \"\"\"\n",
    "    model, env, rgb_on, render_size = model_env_from_path(path)\n",
    "    count_successes = 0\n",
    "    if rgb_on:\n",
    "        env = RGBImgObsWrapper(env, render_size)\n",
    "        env = ImgObsWrapper(env)\n",
    "    else:\n",
    "        env = TensorObsWrapper(env)\n",
    "\n",
    "    env = RandomizeGoalWrapper(env, randomization=1)\n",
    "\n",
    "    episode_lengths = []\n",
    "    success_goal_locations = []\n",
    "    all_goals = []\n",
    "    for _ in range(episodes):\n",
    "        obs = env.reset()\n",
    "        done = False\n",
    "        step = 0\n",
    "        while not done:\n",
    "            action, _ = model.predict(obs, deterministic=True)\n",
    "            obs, _, done, info = env.step(action)\n",
    "            step += 1\n",
    "        if info['is_success']:\n",
    "            count_successes += 1\n",
    "            success_goal_locations += env.goal_positions\n",
    "        all_goals += env.goal_positions\n",
    "        episode_lengths.append(step)\n",
    "    \n",
    "    success_goal_locations = set(success_goal_locations)\n",
    "    all_goals = set(all_goals)\n",
    "    adj_success_rate = len(success_goal_locations)/len(all_goals)\n",
    "\n",
    "    print(\"Goal randomization success rate:\\33[35m\", round(count_successes/episodes*100, 1), f\"\\33[0m% on \\33[35m{episodes}\\33[0m random-goal episodes\")\n",
    "    print(f\"Adjusted goal randomization success rate on unique different goals: \\33[35m{round(adj_success_rate*100, 1)}\\33[0m %\")\n",
    "    print(\"Goal randomization mean episode length:\\33[35m\", np.mean(episode_lengths), \"\\33[0m\")\n",
    "    print(\"Goal positions with successes (unique):\", success_goal_locations)\n",
    "\n",
    "def execute_episode(path: str):\n",
    "    \"\"\"Execute a test episode with the specified saved model on the training environment configuration. Slipping and wall rebound is deactivated. Prints episode summary to stdout.\n",
    "\n",
    "    Args:\n",
    "        path (str): Location of the saved model. Must conform to specified structure (see experiment_config.py)\n",
    "    \"\"\" \n",
    "    model, env, rgb, render_size = model_env_from_path(path)\n",
    "    if rgb:\n",
    "        env = RGBImgObsWrapper(env, tile_size=render_size)\n",
    "        env = ImgObsWrapper(env)\n",
    "    else:\n",
    "        env = TensorObsWrapper(env)\n",
    "\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    count_steps = 0\n",
    "    cumulative_reward = 0\n",
    "    while not done:\n",
    "        action, _ = model.predict(obs, deterministic=True)\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        cumulative_reward += reward\n",
    "        count_steps += 1\n",
    "    print(f\"Episode summary -> success: \\33[35m{info['is_success']}\\33[0m, cumulative reward: \\33[35m{cumulative_reward}\\33[0m, number of steps: \\33[35m{count_steps}\\33[0m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# definitions of helpers & constants to reuse for custom environments\n",
    "\n",
    "base_lava_positions = []\n",
    "for y in range(1, 11 - 1):\n",
    "    base_lava_positions.append((1, y))\n",
    "for y in range(11 - 3, 11 - 8, -1):\n",
    "    base_lava_positions.append((3, y))\n",
    "base_lava_positions.extend([(6, 11 - 5), (6, 11 - 6)])\n",
    "base_lava_positions.remove((1,3)) # remove location of original goal position\n",
    "\n",
    "original_lava = lambda: base_lava_positions.copy()\n",
    "\n",
    "upper_right_goal_env = lambda: make_env(goal_positions=[(9, 1)])\n",
    "alt_upper_right_goal_env = lambda: make_env(goal_positions=[(9, 1)], lava_positions=original_lava())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining standard test suite\n",
    "def test_suite_model(path: str, rnd_eps=50):\n",
    "    print(sinfo, \"Beginning execution of test suite.\")\n",
    "    print(f\"Path: \\33[3m{path[len(model_path_prefix):]}\\33[0m\")\n",
    "\n",
    "    # test agent on deterministic (!) environment\n",
    "    print(\"\\n\\33[4mAgent success on \\33[1mdeterministic\\33[0;4m training environment:\\33[0m\")\n",
    "    execute_episode(path)\n",
    "\n",
    "    # visualized policy on original environment\n",
    "    print(\"\\n\\33[4mPolicy visualization on training environment:\\33[0m\")\n",
    "    print(visualize_policy(path))\n",
    "    \n",
    "    # summary statistics on randomized goal locations\n",
    "    print(\"\\33[4mTesting Goal generalization capabilities:\\33[0m\")\n",
    "    randomized_goal_stats(path, episodes=rnd_eps)\n",
    "\n",
    "    print(sinfo, \"Test suite execution ended.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ignore \"memory not enough\" warnings concerning replay buffer\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', module=\"stable_baselines3.common.buffers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: <object object at 0x136970e50>\n"
     ]
    }
   ],
   "source": [
    "%matplotlib\n",
    "# Force matplotlib to render outside of notebook (Don't use 'inline' backend)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `exp_001`\n",
    "\n",
    "_Environment configuration:_\n",
    "```json\n",
    "    \"exp_001\" : {\n",
    "        \"max_steps\" : 150,\n",
    "        \"slip_proba\" : 0,\n",
    "        \"wall_rebound\" : false,\n",
    "        \"spiky_active\" : false,\n",
    "        \"reward_spec\" : {\n",
    "            \"step_penalty\" : 0,\n",
    "            \"goal_reward\" : 1,\n",
    "            \"absorbing_states\" : false,\n",
    "            \"absorbing_reward_goal\" : 0,\n",
    "            \"absorbing_reward_lava\" : -1,\n",
    "            \"risky_tile_reward\" : 0,\n",
    "            \"lava_reward\" : -1\n",
    "        }\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DQN on `exp_001`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DQN algo_default"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's load one of the successful trained agents on the `stable-baselines3` DQN defaults. It was trained on **tensor observations for 250k time steps**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1 = model_path_prefix + \"exp_001/tensor_obs/dqn/algo_default/seed_763.zip\""
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "test_agent_on_environment(model_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agent walks to the goal tile. Let's visualize it's policy on this version of the environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO:\u001b[0m Beginning execution of test suite.\n",
      "Path: \u001b[3mexp_001/tensor_obs/dqn/algo_default/seed_763.zip\u001b[0m\n",
      "\n",
      "\u001b[4mAgent success on \u001b[1mdeterministic\u001b[0;4m training environment:\u001b[0m\n",
      "Episode summary -> success: \u001b[35mTrue\u001b[0m, cumulative reward: \u001b[35m1\u001b[0m, number of steps: \u001b[35m7\u001b[0m\n",
      "\n",
      "\u001b[4mPolicy visualization on training environment:\u001b[0m\n",
      " 0   1   2   3   4   5   6   7   8   9   10  \n",
      "[\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m]  0 \n",
      "[\u001b[36m#\u001b[0m] [\u001b[41m~\u001b[0m] [v] [<] [<] [<] [v] [<] [<] [<] [\u001b[36m#\u001b[0m]  1 \n",
      "[\u001b[36m#\u001b[0m] [\u001b[41m~\u001b[0m] [v] [<] [<] [v] [v] [v] [<] [<] [\u001b[36m#\u001b[0m]  2 \n",
      "[\u001b[36m#\u001b[0m] [\u001b[42mx\u001b[0m] [<] [<] [<] [<] [<] [<] [<] [<] [\u001b[36m#\u001b[0m]  3 \n",
      "[\u001b[36m#\u001b[0m] [\u001b[41m~\u001b[0m] [^] [\u001b[41m~\u001b[0m] [^] [<] [<] [^] [^] [<] [\u001b[36m#\u001b[0m]  4 \n",
      "[\u001b[36m#\u001b[0m] [\u001b[41m~\u001b[0m] [^] [\u001b[41m~\u001b[0m] [^] [<] [\u001b[41m~\u001b[0m] [^] [<] [>] [\u001b[36m#\u001b[0m]  5 \n",
      "[\u001b[36m#\u001b[0m] [\u001b[41m~\u001b[0m] [^] [\u001b[41m~\u001b[0m] [^] [<] [\u001b[41m~\u001b[0m] [^] [^] [>] [\u001b[36m#\u001b[0m]  6 \n",
      "[\u001b[36m#\u001b[0m] [\u001b[41m~\u001b[0m] [^] [\u001b[41m~\u001b[0m] [^] [<] [<] [<] [<] [<] [\u001b[36m#\u001b[0m]  7 \n",
      "[\u001b[36m#\u001b[0m] [\u001b[41m~\u001b[0m] [^] [\u001b[41m~\u001b[0m] [v] [v] [<] [<] [<] [<] [\u001b[36m#\u001b[0m]  8 \n",
      "[\u001b[36m#\u001b[0m] [\u001b[41m~\u001b[0m] [^] [<] [<] [<] [<] [<] [<] [<] [\u001b[36m#\u001b[0m]  9 \n",
      "[\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m]  10 \n",
      "\n",
      "\u001b[4mTesting Goal generalization capabilities:\u001b[0m\n",
      "Goal randomization success rate:\u001b[35m 0.0 \u001b[0m% on \u001b[35m50\u001b[0m random-goal episodes\n",
      "Adjusted goal randomization success rate on unique different goals: \u001b[35m0.0\u001b[0m %\n",
      "Goal randomization mean episode length:\u001b[35m 147.14 \u001b[0m\n",
      "Goal positions with successes (unique): set()\n",
      "\u001b[32mINFO:\u001b[0m Test suite execution ended.\n"
     ]
    }
   ],
   "source": [
    "test_suite_model(model_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation of output:**\n",
    "\n",
    "- `[~]` is lava\n",
    "- `[#]` is a wall\n",
    "- `[x]` is the goal tile\n",
    "- `<,^,>,v` are the directions the agent would take from that cell\n",
    "\n",
    "The agent successfully navigates the environment from most positions, it quickly finds the goal tile and mostly doesn't take any detours. Exceptions are (9,5), (9,6), where the agent prefers to move against the wall (in this case not moving at all). The agent does not always walk the quickest paht, but considering that the reward model of `exp_001` does not incentivize the agent to find the shortest path (no time penalty), this is not especially surprising.\n",
    "One can also observe that in no case would the agent walk in one of the lava tiles.\n",
    "\n",
    "From the goal randomization summary, it can be concluded that the agent has no generalization capabilities whatsoever. In 50 episodes with random goal tile placements (guaranteed to be accessible), the agent success rate is 0.\n",
    "\n",
    "As an example, the next cell shows the agent interaction when the goal tile is placed at the top right corner:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "test_agent_on_environment(\n",
    "    model_1,\n",
    "    num_episodes=1,\n",
    "    custom_environment=upper_right_goal_env()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0   1   2   3   4   5   6   7   8   9   10  \n",
      "[\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m]  0 \n",
      "[\u001b[36m#\u001b[0m] [\u001b[41m~\u001b[0m] [v] [<] [<] [<] [<] [<] [>] [\u001b[42mx\u001b[0m] [\u001b[36m#\u001b[0m]  1 \n",
      "[\u001b[36m#\u001b[0m] [\u001b[41m~\u001b[0m] [v] [<] [<] [<] [<] [<] [<] [<] [\u001b[36m#\u001b[0m]  2 \n",
      "[\u001b[36m#\u001b[0m] [\u001b[41m~\u001b[0m] [>] [<] [<] [<] [<] [<] [<] [<] [\u001b[36m#\u001b[0m]  3 \n",
      "[\u001b[36m#\u001b[0m] [\u001b[41m~\u001b[0m] [v] [\u001b[41m~\u001b[0m] [^] [<] [<] [<] [<] [>] [\u001b[36m#\u001b[0m]  4 \n",
      "[\u001b[36m#\u001b[0m] [\u001b[41m~\u001b[0m] [v] [\u001b[41m~\u001b[0m] [^] [<] [\u001b[41m~\u001b[0m] [>] [>] [>] [\u001b[36m#\u001b[0m]  5 \n",
      "[\u001b[36m#\u001b[0m] [\u001b[41m~\u001b[0m] [v] [\u001b[41m~\u001b[0m] [^] [<] [\u001b[41m~\u001b[0m] [>] [>] [>] [\u001b[36m#\u001b[0m]  6 \n",
      "[\u001b[36m#\u001b[0m] [\u001b[41m~\u001b[0m] [v] [\u001b[41m~\u001b[0m] [>] [<] [>] [>] [>] [>] [\u001b[36m#\u001b[0m]  7 \n",
      "[\u001b[36m#\u001b[0m] [\u001b[41m~\u001b[0m] [^] [\u001b[41m~\u001b[0m] [v] [<] [>] [>] [>] [>] [\u001b[36m#\u001b[0m]  8 \n",
      "[\u001b[36m#\u001b[0m] [\u001b[41m~\u001b[0m] [>] [<] [<] [<] [<] [>] [>] [>] [\u001b[36m#\u001b[0m]  9 \n",
      "[\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m]  10 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(visualize_policy(model_1, upper_right_goal_env()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The agent oscillates between the starting position (2,9) and the adjacent tile to the right. It seems confused about the changed environment. Two factors seem to come into play: The goal tile was changed to (9,9) and the old goal tile was replaced with a lava tile. The agent might have recognized that its previous strategy might no longer be safe. Given the policy visualization for this environment version, this is further evidenced by the fact that the agent still tries to avoid all lava tiles, even the new lava tile at (1,3).\n",
    "Let's see how the agent behaviour changes when the original goal tile is turned to floor."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "test_agent_on_environment(\n",
    "    model_1,\n",
    "    num_episodes=1,\n",
    "    custom_environment=alt_upper_right_goal_env()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0   1   2   3   4   5   6   7   8   9   10  \n",
      "[\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m]  0 \n",
      "[\u001b[36m#\u001b[0m] [\u001b[41m~\u001b[0m] [v] [<] [<] [<] [<] [<] [<] [\u001b[42mx\u001b[0m] [\u001b[36m#\u001b[0m]  1 \n",
      "[\u001b[36m#\u001b[0m] [\u001b[41m~\u001b[0m] [v] [<] [<] [<] [<] [<] [<] [<] [\u001b[36m#\u001b[0m]  2 \n",
      "[\u001b[36m#\u001b[0m] [v] [<] [<] [<] [<] [<] [<] [<] [<] [\u001b[36m#\u001b[0m]  3 \n",
      "[\u001b[36m#\u001b[0m] [\u001b[41m~\u001b[0m] [^] [\u001b[41m~\u001b[0m] [^] [<] [<] [<] [<] [<] [\u001b[36m#\u001b[0m]  4 \n",
      "[\u001b[36m#\u001b[0m] [\u001b[41m~\u001b[0m] [^] [\u001b[41m~\u001b[0m] [^] [<] [\u001b[41m~\u001b[0m] [>] [<] [>] [\u001b[36m#\u001b[0m]  5 \n",
      "[\u001b[36m#\u001b[0m] [\u001b[41m~\u001b[0m] [^] [\u001b[41m~\u001b[0m] [^] [<] [\u001b[41m~\u001b[0m] [>] [>] [>] [\u001b[36m#\u001b[0m]  6 \n",
      "[\u001b[36m#\u001b[0m] [\u001b[41m~\u001b[0m] [^] [\u001b[41m~\u001b[0m] [^] [<] [>] [<] [<] [>] [\u001b[36m#\u001b[0m]  7 \n",
      "[\u001b[36m#\u001b[0m] [\u001b[41m~\u001b[0m] [^] [\u001b[41m~\u001b[0m] [v] [<] [<] [<] [>] [>] [\u001b[36m#\u001b[0m]  8 \n",
      "[\u001b[36m#\u001b[0m] [\u001b[41m~\u001b[0m] [>] [<] [<] [<] [<] [<] [>] [>] [\u001b[36m#\u001b[0m]  9 \n",
      "[\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m]  10 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(visualize_policy(model_1, custom_env=alt_upper_right_goal_env()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the agent still oscillates at the starting position and its right neighbour, however, its policy would lead the agent to walk to the original goal position when placed in most other cells (and then terminating the episode by walking in the lava tile below). Note that no policy-induced trajectory would end up in the actual goal tile given this environment.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Next, I'd like to see if the agent is able to avoid _newly placed_ lava tiles. Let's see two examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0   1   2   3   4   5   6   7   8   9   10  \n",
      "[\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m]  0 \n",
      "[\u001b[36m#\u001b[0m] [\u001b[41m~\u001b[0m] [v] [<] [<] [<] [<] [<] [<] [<] [\u001b[36m#\u001b[0m]  1 \n",
      "[\u001b[36m#\u001b[0m] [\u001b[41m~\u001b[0m] [v] [<] [<] [<] [<] [<] [<] [<] [\u001b[36m#\u001b[0m]  2 \n",
      "[\u001b[36m#\u001b[0m] [\u001b[42mx\u001b[0m] [<] [<] [<] [<] [<] [<] [<] [<] [\u001b[36m#\u001b[0m]  3 \n",
      "[\u001b[36m#\u001b[0m] [\u001b[41m~\u001b[0m] [^] [\u001b[41m~\u001b[0m] [^] [<] [<] [<] [<] [<] [\u001b[36m#\u001b[0m]  4 \n",
      "[\u001b[36m#\u001b[0m] [\u001b[41m~\u001b[0m] [v] [\u001b[41m~\u001b[0m] [^] [<] [\u001b[41m~\u001b[0m] [^] [<] [<] [\u001b[36m#\u001b[0m]  5 \n",
      "[\u001b[36m#\u001b[0m] [\u001b[41m~\u001b[0m] [^] [\u001b[41m~\u001b[0m] [^] [<] [\u001b[41m~\u001b[0m] [^] [<] [<] [\u001b[36m#\u001b[0m]  6 \n",
      "[\u001b[36m#\u001b[0m] [\u001b[41m~\u001b[0m] [v] [\u001b[41m~\u001b[0m] [^] [<] [<] [<] [<] [<] [\u001b[36m#\u001b[0m]  7 \n",
      "[\u001b[36m#\u001b[0m] [\u001b[41m~\u001b[0m] [\u001b[41m~\u001b[0m] [\u001b[41m~\u001b[0m] [v] [<] [<] [<] [<] [<] [\u001b[36m#\u001b[0m]  8 \n",
      "[\u001b[36m#\u001b[0m] [\u001b[41m~\u001b[0m] [^] [<] [<] [<] [<] [<] [<] [<] [\u001b[36m#\u001b[0m]  9 \n",
      "[\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m]  10 \n",
      "\n",
      " 0   1   2   3   4   5   6   7   8   9   10  \n",
      "[\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m]  0 \n",
      "[\u001b[36m#\u001b[0m] [\u001b[41m~\u001b[0m] [v] [<] [<] [v] [v] [v] [^] [^] [\u001b[36m#\u001b[0m]  1 \n",
      "[\u001b[36m#\u001b[0m] [\u001b[41m~\u001b[0m] [v] [<] [<] [v] [v] [v] [v] [v] [\u001b[36m#\u001b[0m]  2 \n",
      "[\u001b[36m#\u001b[0m] [\u001b[42mx\u001b[0m] [v] [<] [<] [<] [<] [<] [<] [<] [\u001b[36m#\u001b[0m]  3 \n",
      "[\u001b[36m#\u001b[0m] [\u001b[41m~\u001b[0m] [\u001b[41m~\u001b[0m] [\u001b[41m~\u001b[0m] [^] [<] [<] [^] [^] [^] [\u001b[36m#\u001b[0m]  4 \n",
      "[\u001b[36m#\u001b[0m] [\u001b[41m~\u001b[0m] [^] [\u001b[41m~\u001b[0m] [^] [<] [\u001b[41m~\u001b[0m] [^] [^] [^] [\u001b[36m#\u001b[0m]  5 \n",
      "[\u001b[36m#\u001b[0m] [\u001b[41m~\u001b[0m] [^] [\u001b[41m~\u001b[0m] [^] [^] [\u001b[41m~\u001b[0m] [^] [^] [^] [\u001b[36m#\u001b[0m]  6 \n",
      "[\u001b[36m#\u001b[0m] [\u001b[41m~\u001b[0m] [^] [\u001b[41m~\u001b[0m] [^] [^] [<] [^] [^] [^] [\u001b[36m#\u001b[0m]  7 \n",
      "[\u001b[36m#\u001b[0m] [\u001b[41m~\u001b[0m] [^] [\u001b[41m~\u001b[0m] [v] [v] [^] [^] [^] [^] [\u001b[36m#\u001b[0m]  8 \n",
      "[\u001b[36m#\u001b[0m] [\u001b[41m~\u001b[0m] [^] [<] [<] [<] [<] [^] [^] [^] [\u001b[36m#\u001b[0m]  9 \n",
      "[\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m]  10 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "blocking_lava = original_lava() + [(2,8)]\n",
    "env = make_env(\n",
    "    lava_positions=blocking_lava\n",
    ")\n",
    "print(visualize_policy(model_1, custom_env=env))\n",
    "\n",
    "blocking_lava = original_lava() + [(2,4)]\n",
    "env = make_env(\n",
    "    lava_positions=blocking_lava\n",
    ")\n",
    "print(visualize_policy(model_1, custom_env=env))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that the agent is not able to grasp the inherent danger of lava tiles. It only evades the lava tile positions that it already knows from training. It seems the agent has not learned the causation between lava and negative reward but instead learned the correlation between the positions (of lava tiles during training) in the gridworld and the negative reward. This would also explain why changing the starting position does not confuse the agent in searching the goal tile (when it is at the original position). During training, only one part of the observation tensor is constantly changing, namely the agent's position."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Summary:**\n",
    "\n",
    "- During training, the agent learned to walk to the goal tile successfully\n",
    "- When placed in the training environment, the trained model is able to find the goal tile from almost all starting positions\n",
    "- However, the agent is not able to generalize this knowledge to goal tiles with other positions\n",
    "- Only lava tiles known from training are circumvented by the agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DQN dqn_slow_learning\n",
    "\n",
    "Next, let's load `dqn_slow_learning` on exp_001. It was trained for 1m timesteps and is taken as a negative example. Model performance at the end of the training was a bit below 0. After some surges in performance around 200k-300k time steps, the agents across all random seeds forget their initial performance and slowly converge to a local minimum around 0. One such policy is visualized below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2 = model_path_prefix + \"exp_001/tensor_obs/dqn/dqn_slow_learning/seed_4744.zip\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO:\u001b[0m Beginning execution of test suite.\n",
      "Path: \u001b[3mexp_001/tensor_obs/dqn/dqn_slow_learning/seed_4744.zip\u001b[0m\n",
      "\n",
      "\u001b[4mAgent success on \u001b[1mdeterministic\u001b[0;4m training environment:\u001b[0m\n",
      "Episode summary -> success: \u001b[35mFalse\u001b[0m, cumulative reward: \u001b[35m0\u001b[0m, number of steps: \u001b[35m150\u001b[0m\n",
      "\n",
      "\u001b[4mPolicy visualization on training environment:\u001b[0m\n",
      " 0   1   2   3   4   5   6   7   8   9   10  \n",
      "[\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m]  0 \n",
      "[\u001b[36m#\u001b[0m] [\u001b[41m~\u001b[0m] [>] [>] [>] [>] [>] [v] [>] [>] [\u001b[36m#\u001b[0m]  1 \n",
      "[\u001b[36m#\u001b[0m] [\u001b[41m~\u001b[0m] [>] [>] [>] [>] [>] [>] [>] [>] [\u001b[36m#\u001b[0m]  2 \n",
      "[\u001b[36m#\u001b[0m] [\u001b[42mx\u001b[0m] [^] [>] [>] [>] [>] [>] [>] [>] [\u001b[36m#\u001b[0m]  3 \n",
      "[\u001b[36m#\u001b[0m] [\u001b[41m~\u001b[0m] [^] [\u001b[41m~\u001b[0m] [>] [>] [^] [>] [>] [>] [\u001b[36m#\u001b[0m]  4 \n",
      "[\u001b[36m#\u001b[0m] [\u001b[41m~\u001b[0m] [^] [\u001b[41m~\u001b[0m] [>] [^] [\u001b[41m~\u001b[0m] [>] [>] [>] [\u001b[36m#\u001b[0m]  5 \n",
      "[\u001b[36m#\u001b[0m] [\u001b[41m~\u001b[0m] [v] [\u001b[41m~\u001b[0m] [>] [v] [\u001b[41m~\u001b[0m] [>] [>] [>] [\u001b[36m#\u001b[0m]  6 \n",
      "[\u001b[36m#\u001b[0m] [\u001b[41m~\u001b[0m] [v] [\u001b[41m~\u001b[0m] [>] [>] [>] [>] [>] [>] [\u001b[36m#\u001b[0m]  7 \n",
      "[\u001b[36m#\u001b[0m] [\u001b[41m~\u001b[0m] [v] [\u001b[41m~\u001b[0m] [>] [>] [>] [>] [>] [>] [\u001b[36m#\u001b[0m]  8 \n",
      "[\u001b[36m#\u001b[0m] [\u001b[41m~\u001b[0m] [>] [>] [>] [>] [>] [>] [>] [>] [\u001b[36m#\u001b[0m]  9 \n",
      "[\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m]  10 \n",
      "\n",
      "\u001b[4mTesting Goal generalization capabilities:\u001b[0m\n",
      "Goal randomization success rate:\u001b[35m 14.0 \u001b[0m% on \u001b[35m50\u001b[0m random-goal episodes\n",
      "Adjusted goal randomization success rate on unique different goals: \u001b[35m14.3\u001b[0m %\n",
      "Goal randomization mean episode length:\u001b[35m 129.5 \u001b[0m\n",
      "Goal positions with successes (unique): {(2, 7), (2, 3), (2, 2), (2, 5), (2, 8)}\n",
      "\u001b[32mINFO:\u001b[0m Test suite execution ended.\n"
     ]
    }
   ],
   "source": [
    "test_suite_model(model_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly, the agent seems to be able to find goals when they are on the x=2 axis. However, this can hardly be labelled as generalization capability given that the agent is not even able to solve the environment for which it was trained. (However, it is possible that the neural network attributes some relevance to placement of goal tiles.) This is not further investigated due to the reason stated above."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "test_agent_on_environment(model_2, custom_environment=make_env(goal_positions=[(2,1)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Goal randomization\n",
    "\n",
    "Goal randomization with 2% random-goal episodes\n",
    "\n",
    "```json\n",
    "    \"exp_001_goal_rnd_2\" : {\n",
    "        \"max_steps\" : 150,\n",
    "        \"slip_proba\" : 0,\n",
    "        \"wall_rebound\" : false,\n",
    "        \"spiky_active\" : false,\n",
    "        \"reward_spec\" : {\n",
    "            \"step_penalty\" : 0,\n",
    "            \"goal_reward\" : 1,\n",
    "            \"absorbing_states\" : false,\n",
    "            \"absorbing_reward_goal\" : 0,\n",
    "            \"absorbing_reward_lava\" : -1,\n",
    "            \"risky_tile_reward\" : 0,\n",
    "            \"lava_reward\" : -1\n",
    "        },\n",
    "        \"goal_rnd\" : 0.02\n",
    "    }\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_3 = model_path_prefix + \"exp_001_goal_rnd_2/tensor_obs/dqn/dqn_low_eps/seed_5672.zip\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO:\u001b[0m Beginning execution of test suite.\n",
      "Path: \u001b[3mexp_001_goal_rnd_2/tensor_obs/dqn/dqn_low_eps/seed_5672.zip\u001b[0m\n",
      "\n",
      "\u001b[4mAgent success on \u001b[1mdeterministic\u001b[0;4m training environment:\u001b[0m\n",
      "Episode summary -> success: \u001b[35mTrue\u001b[0m, cumulative reward: \u001b[35m1\u001b[0m, number of steps: \u001b[35m7\u001b[0m\n",
      "\n",
      "\u001b[4mPolicy visualization on training environment:\u001b[0m\n",
      " 0   1   2   3   4   5   6   7   8   9   10  \n",
      "[\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m]  0 \n",
      "[\u001b[36m#\u001b[0m] [\u001b[41m~\u001b[0m] [^] [<] [<] [<] [>] [v] [<] [<] [\u001b[36m#\u001b[0m]  1 \n",
      "[\u001b[36m#\u001b[0m] [\u001b[41m~\u001b[0m] [>] [<] [<] [<] [<] [<] [v] [<] [\u001b[36m#\u001b[0m]  2 \n",
      "[\u001b[36m#\u001b[0m] [\u001b[42mx\u001b[0m] [<] [>] [<] [<] [<] [<] [<] [<] [\u001b[36m#\u001b[0m]  3 \n",
      "[\u001b[36m#\u001b[0m] [\u001b[41m~\u001b[0m] [^] [\u001b[41m~\u001b[0m] [^] [<] [>] [<] [<] [<] [\u001b[36m#\u001b[0m]  4 \n",
      "[\u001b[36m#\u001b[0m] [\u001b[41m~\u001b[0m] [^] [\u001b[41m~\u001b[0m] [^] [^] [\u001b[41m~\u001b[0m] [^] [<] [<] [\u001b[36m#\u001b[0m]  5 \n",
      "[\u001b[36m#\u001b[0m] [\u001b[41m~\u001b[0m] [^] [\u001b[41m~\u001b[0m] [^] [^] [\u001b[41m~\u001b[0m] [^] [<] [>] [\u001b[36m#\u001b[0m]  6 \n",
      "[\u001b[36m#\u001b[0m] [\u001b[41m~\u001b[0m] [^] [\u001b[41m~\u001b[0m] [^] [<] [>] [<] [<] [<] [\u001b[36m#\u001b[0m]  7 \n",
      "[\u001b[36m#\u001b[0m] [\u001b[41m~\u001b[0m] [^] [\u001b[41m~\u001b[0m] [^] [<] [>] [>] [<] [>] [\u001b[36m#\u001b[0m]  8 \n",
      "[\u001b[36m#\u001b[0m] [\u001b[41m~\u001b[0m] [^] [>] [>] [>] [<] [<] [<] [<] [\u001b[36m#\u001b[0m]  9 \n",
      "[\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m]  10 \n",
      "\n",
      "\u001b[4mTesting Goal generalization capabilities:\u001b[0m\n",
      "Goal randomization success rate:\u001b[35m 18.0 \u001b[0m% on \u001b[35m100\u001b[0m random-goal episodes\n",
      "Adjusted goal randomization success rate on unique different goals: \u001b[35m16.0\u001b[0m %\n",
      "Goal randomization mean episode length:\u001b[35m 70.74 \u001b[0m\n",
      "Goal positions with successes (unique): {(2, 4), (2, 7), (2, 3), (2, 6), (2, 2), (5, 9), (2, 5), (2, 8)}\n",
      "\u001b[32mINFO:\u001b[0m Test suite execution ended.\n"
     ]
    }
   ],
   "source": [
    "test_suite_model(model_3, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [ ] Hierzu Notizen aufschreiben"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing the best model:\n",
    "model_3_best = model_path_prefix + \"exp_001_goal_rnd_2/tensor_obs/dqn/dqn_low_eps/seed_4744_best_model/best_model.zip\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO:\u001b[0m Beginning execution of test suite.\n",
      "Path: \u001b[3mexp_001_goal_rnd_2/tensor_obs/dqn/dqn_low_eps/seed_4744_best_model/best_model.zip\u001b[0m\n",
      "\n",
      "\u001b[4mAgent success on \u001b[1mdeterministic\u001b[0;4m training environment:\u001b[0m\n",
      "Episode summary -> success: \u001b[35mTrue\u001b[0m, cumulative reward: \u001b[35m1\u001b[0m, number of steps: \u001b[35m7\u001b[0m\n",
      "\n",
      "\u001b[4mPolicy visualization on training environment:\u001b[0m\n",
      " 0   1   2   3   4   5   6   7   8   9   10  \n",
      "[\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m]  0 \n",
      "[\u001b[36m#\u001b[0m] [\u001b[41m~\u001b[0m] [v] [<] [<] [v] [<] [v] [^] [<] [\u001b[36m#\u001b[0m]  1 \n",
      "[\u001b[36m#\u001b[0m] [\u001b[41m~\u001b[0m] [v] [<] [<] [<] [<] [>] [<] [<] [\u001b[36m#\u001b[0m]  2 \n",
      "[\u001b[36m#\u001b[0m] [\u001b[42mx\u001b[0m] [<] [>] [<] [<] [^] [<] [>] [<] [\u001b[36m#\u001b[0m]  3 \n",
      "[\u001b[36m#\u001b[0m] [\u001b[41m~\u001b[0m] [^] [\u001b[41m~\u001b[0m] [>] [<] [>] [<] [<] [^] [\u001b[36m#\u001b[0m]  4 \n",
      "[\u001b[36m#\u001b[0m] [\u001b[41m~\u001b[0m] [^] [\u001b[41m~\u001b[0m] [^] [^] [\u001b[41m~\u001b[0m] [^] [v] [<] [\u001b[36m#\u001b[0m]  5 \n",
      "[\u001b[36m#\u001b[0m] [\u001b[41m~\u001b[0m] [^] [\u001b[41m~\u001b[0m] [>] [<] [\u001b[41m~\u001b[0m] [>] [<] [<] [\u001b[36m#\u001b[0m]  6 \n",
      "[\u001b[36m#\u001b[0m] [\u001b[41m~\u001b[0m] [^] [\u001b[41m~\u001b[0m] [>] [v] [<] [>] [<] [<] [\u001b[36m#\u001b[0m]  7 \n",
      "[\u001b[36m#\u001b[0m] [\u001b[41m~\u001b[0m] [^] [\u001b[41m~\u001b[0m] [>] [^] [<] [<] [<] [<] [\u001b[36m#\u001b[0m]  8 \n",
      "[\u001b[36m#\u001b[0m] [\u001b[41m~\u001b[0m] [^] [<] [<] [<] [<] [<] [<] [<] [\u001b[36m#\u001b[0m]  9 \n",
      "[\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m]  10 \n",
      "\n",
      "\u001b[4mTesting Goal generalization capabilities:\u001b[0m\n",
      "Goal randomization success rate:\u001b[35m 16.0 \u001b[0m% on \u001b[35m100\u001b[0m random-goal episodes\n",
      "Adjusted goal randomization success rate on unique different goals: \u001b[35m14.0\u001b[0m %\n",
      "Goal randomization mean episode length:\u001b[35m 126.49 \u001b[0m\n",
      "Goal positions with successes (unique): {(2, 4), (2, 7), (2, 3), (3, 9), (2, 6), (2, 5), (2, 8)}\n",
      "\u001b[32mINFO:\u001b[0m Test suite execution ended.\n"
     ]
    }
   ],
   "source": [
    "test_suite_model(model_3_best, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Goal randomization with 5 % random-goal episodes:\n",
    "\n",
    "```json\n",
    "    \"exp_001_goal_rnd_5\" : {\n",
    "        \"max_steps\" : 150,\n",
    "        \"slip_proba\" : 0,\n",
    "        \"wall_rebound\" : false,\n",
    "        \"spiky_active\" : false,\n",
    "        \"reward_spec\" : {\n",
    "            \"step_penalty\" : 0,\n",
    "            \"goal_reward\" : 1,\n",
    "            \"absorbing_states\" : false,\n",
    "            \"absorbing_reward_goal\" : 0,\n",
    "            \"absorbing_reward_lava\" : -1,\n",
    "            \"risky_tile_reward\" : 0,\n",
    "            \"lava_reward\" : -1\n",
    "        },\n",
    "        \"goal_rnd\" : 0.05\n",
    "    }\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [ ] Hier weitermachen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_4 = model_path_prefix + \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A2C on `exp_001`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load an a2c model that was very succesful during training. It was trained for 500k time steps The next observation is an interesting one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO:\u001b[0m Beginning execution of test suite.\n",
      "Path: \u001b[3mexp_001/tensor_obs/a2c/a2c_entropy_6/seed_4267.zip\u001b[0m\n",
      "\n",
      "\u001b[4mAgent success on \u001b[1mdeterministic\u001b[0;4m training environment:\u001b[0m\n",
      "Episode summary -> success: \u001b[35mTrue\u001b[0m, cumulative reward: \u001b[35m1\u001b[0m, number of steps: \u001b[35m7\u001b[0m\n",
      "\n",
      "\u001b[4mPolicy visualization on training environment:\u001b[0m\n",
      " 0   1   2   3   4   5   6   7   8   9   10  \n",
      "[\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m]  0 \n",
      "[\u001b[36m#\u001b[0m] [\u001b[41m~\u001b[0m] [v] [^] [^] [^] [^] [^] [^] [^] [\u001b[36m#\u001b[0m]  1 \n",
      "[\u001b[36m#\u001b[0m] [\u001b[41m~\u001b[0m] [v] [<] [^] [^] [^] [^] [^] [^] [\u001b[36m#\u001b[0m]  2 \n",
      "[\u001b[36m#\u001b[0m] [\u001b[42mx\u001b[0m] [<] [^] [^] [^] [^] [^] [^] [^] [\u001b[36m#\u001b[0m]  3 \n",
      "[\u001b[36m#\u001b[0m] [\u001b[41m~\u001b[0m] [^] [\u001b[41m~\u001b[0m] [^] [^] [^] [^] [^] [^] [\u001b[36m#\u001b[0m]  4 \n",
      "[\u001b[36m#\u001b[0m] [\u001b[41m~\u001b[0m] [^] [\u001b[41m~\u001b[0m] [^] [^] [\u001b[41m~\u001b[0m] [^] [^] [^] [\u001b[36m#\u001b[0m]  5 \n",
      "[\u001b[36m#\u001b[0m] [\u001b[41m~\u001b[0m] [^] [\u001b[41m~\u001b[0m] [^] [^] [\u001b[41m~\u001b[0m] [^] [^] [^] [\u001b[36m#\u001b[0m]  6 \n",
      "[\u001b[36m#\u001b[0m] [\u001b[41m~\u001b[0m] [^] [\u001b[41m~\u001b[0m] [^] [^] [^] [^] [^] [^] [\u001b[36m#\u001b[0m]  7 \n",
      "[\u001b[36m#\u001b[0m] [\u001b[41m~\u001b[0m] [^] [\u001b[41m~\u001b[0m] [^] [^] [^] [^] [^] [^] [\u001b[36m#\u001b[0m]  8 \n",
      "[\u001b[36m#\u001b[0m] [\u001b[41m~\u001b[0m] [^] [<] [^] [^] [^] [^] [^] [^] [\u001b[36m#\u001b[0m]  9 \n",
      "[\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m]  10 \n",
      "\n",
      "\u001b[4mTesting Goal generalization capabilities:\u001b[0m\n",
      "Goal randomization success rate:\u001b[35m 14.0 \u001b[0m% on \u001b[35m50\u001b[0m random-goal episodes\n",
      "Adjusted goal randomization success rate on unique different goals: \u001b[35m14.3\u001b[0m %\n",
      "Goal randomization mean episode length:\u001b[35m 6.48 \u001b[0m\n",
      "Goal positions with successes (unique): {(2, 4), (2, 7), (2, 3), (2, 5), (2, 8)}\n",
      "\u001b[32mINFO:\u001b[0m Test suite execution ended.\n"
     ]
    }
   ],
   "source": [
    "a2c_model_1 = model_path_prefix + \"exp_001/tensor_obs/a2c/a2c_entropy_6/seed_4267.zip\"\n",
    "\n",
    "test_suite_model(a2c_model_1)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "test_agent_on_environment(\n",
    "    a2c_model_1,\n",
    "    custom_environment=make_env(\n",
    "        goal_positions=[(3,3)],\n",
    "        lava_positions=original_lava()\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Distributional Shift, Goal Misgeneralization:**\n",
    "The agent does not understand that the important tile is the goal tile. It still navigates to the position in which it recieved positive rewards during training. Once the state distribution shifts, the agent is not able to apply the learned skills to a simple alteration of the environment. This shows that the state representation during training is not truly sufficient if we want the agent to be able to generalize knowledge. Perfect example of goal misgeneralization: agent has learned a *directional/location proxy* of the intended objective which is to find the goal tile. What is even more interesting: the agent would walk upwards from almost all other positions, which means it would not even be able to generalize its skills to other starting positions, as the DQN agent could.\n",
    "\n",
    "- This very poor generalization might be a good showcase of how the different algorithms work and learn: When looking at the mean episodic reward during the a2c agent's training, one immediately recognizes that the abruptly gets better around 250k-275k time steps (across all 5 random seeds) \n",
    "- [ ] Then analysis with epsilon-greedy exploration vs actor-critic entropy update and local optimum in which it stays in a very stable manner \n",
    "\n",
    "**Note:** Goal randomization reports a success rate of 14 %. This is expected and does not hint at possible goal generalization capabilities. 14% implies 7 successes out of 50 episodes and we know that the agent always takes the same path to the same location. There are 7 tiles on that path that can be subject to goal randomization (63 tiles are eligible for goal randomization in this setting) which means that we would expect ~11% of the random goals to land on this path. As such, 14% is not a significant deviation. We also see that the mean episode length is 6.48, which implies the agent was stopped early on its intended route which takes 7 steps (until in lava, when goal is not at original location)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Goal Randomization on exp_001\n",
    "\n",
    "The next model is an a2c-agent trained on the entropy_6 + exp_001_goal_rnd_2 configuration for 1m time steps on pixel observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "pixel_model_rnd2 = model_path_prefix + \"exp_001_goal_rnd_2/pixel_obs_8/a2c/a2c_entropy_6/seed_3377.zip\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO:\u001b[0m Beginning execution of test suite.\n",
      "Path: \u001b[3mexp_001_goal_rnd_2/pixel_obs_8/a2c/a2c_entropy_6/seed_3377.zip\u001b[0m\n",
      "\n",
      "\u001b[4mAgent success on \u001b[1mdeterministic\u001b[0;4m training environment:\u001b[0m\n",
      "Episode summary -> success: \u001b[35mTrue\u001b[0m, cumulative reward: \u001b[35m1\u001b[0m, number of steps: \u001b[35m7\u001b[0m\n",
      "\n",
      "\u001b[4mPolicy visualization on training environment:\u001b[0m\n",
      " 0   1   2   3   4   5   6   7   8   9   10  \n",
      "[\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m]  0 \n",
      "[\u001b[36m#\u001b[0m] [\u001b[41m~\u001b[0m] [v] [v] [<] [<] [<] [<] [^] [^] [\u001b[36m#\u001b[0m]  1 \n",
      "[\u001b[36m#\u001b[0m] [\u001b[41m~\u001b[0m] [v] [v] [<] [<] [<] [<] [^] [^] [\u001b[36m#\u001b[0m]  2 \n",
      "[\u001b[36m#\u001b[0m] [\u001b[42mx\u001b[0m] [<] [<] [<] [<] [<] [<] [<] [^] [\u001b[36m#\u001b[0m]  3 \n",
      "[\u001b[36m#\u001b[0m] [\u001b[41m~\u001b[0m] [^] [\u001b[41m~\u001b[0m] [^] [<] [<] [^] [<] [^] [\u001b[36m#\u001b[0m]  4 \n",
      "[\u001b[36m#\u001b[0m] [\u001b[41m~\u001b[0m] [^] [\u001b[41m~\u001b[0m] [^] [^] [\u001b[41m~\u001b[0m] [^] [<] [<] [\u001b[36m#\u001b[0m]  5 \n",
      "[\u001b[36m#\u001b[0m] [\u001b[41m~\u001b[0m] [^] [\u001b[41m~\u001b[0m] [^] [^] [\u001b[41m~\u001b[0m] [^] [<] [<] [\u001b[36m#\u001b[0m]  6 \n",
      "[\u001b[36m#\u001b[0m] [\u001b[41m~\u001b[0m] [^] [\u001b[41m~\u001b[0m] [^] [<] [<] [<] [<] [<] [\u001b[36m#\u001b[0m]  7 \n",
      "[\u001b[36m#\u001b[0m] [\u001b[41m~\u001b[0m] [^] [\u001b[41m~\u001b[0m] [^] [<] [<] [<] [<] [<] [\u001b[36m#\u001b[0m]  8 \n",
      "[\u001b[36m#\u001b[0m] [\u001b[41m~\u001b[0m] [^] [<] [<] [^] [<] [^] [^] [^] [\u001b[36m#\u001b[0m]  9 \n",
      "[\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m]  10 \n",
      "\n",
      "\u001b[4mTesting Goal generalization capabilities:\u001b[0m\n",
      "Goal randomization success rate:\u001b[35m 20.0 \u001b[0m% on \u001b[35m50\u001b[0m random-goal episodes\n",
      "Adjusted goal randomization success rate on unique different goals: \u001b[35m20.0\u001b[0m %\n",
      "Goal randomization mean episode length:\u001b[35m 121.16 \u001b[0m\n",
      "Goal positions with successes (unique): {(2, 4), (7, 1), (2, 7), (4, 2), (2, 5), (4, 1), (2, 8)}\n",
      "\u001b[32mINFO:\u001b[0m Test suite execution ended.\n"
     ]
    }
   ],
   "source": [
    "test_suite_model(pixel_model_rnd2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what generalization capabilities have been developed across different random seeds:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Cell put in raw mode due to high computational cost\n",
    "# (output should always be identical due to random seed resetting)\n",
    "m1 = model_path_prefix + \"exp_001_goal_rnd_2/pixel_obs_8/a2c/a2c_entropy_6/seed_3377.zip\"\n",
    "m2 = model_path_prefix + \"exp_001_goal_rnd_2/pixel_obs_8/a2c/a2c_entropy_6/seed_763.zip\"\n",
    "m3 = model_path_prefix + \"exp_001_goal_rnd_2/pixel_obs_8/a2c/a2c_entropy_6/seed_5672.zip\"\n",
    "m4 = model_path_prefix + \"exp_001_goal_rnd_2/pixel_obs_8/a2c/a2c_entropy_6/seed_4744.zip\"\n",
    "m5 = model_path_prefix + \"exp_001_goal_rnd_2/pixel_obs_8/a2c/a2c_entropy_6/seed_4267.zip\"\n",
    "print(\"---\", m1.split(\"/\")[-1], \"---\")\n",
    "randomized_goal_stats(m1)\n",
    "print(\"---\", m2.split(\"/\")[-1], \"---\")\n",
    "randomized_goal_stats(m2)\n",
    "print(\"---\", m3.split(\"/\")[-1], \"---\")\n",
    "randomized_goal_stats(m3)\n",
    "print(\"---\", m4.split(\"/\")[-1], \"---\")\n",
    "randomized_goal_stats(m4)\n",
    "print(\"---\", m5.split(\"/\")[-1], \"---\")\n",
    "randomized_goal_stats(m5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`exp_001_goal_rnd_5`\n",
    "Let's test randomization capabilities with 5 % randomization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- seed_3377.zip ---\n",
      "Goal randomization success rate:\u001b[35m 30.0 \u001b[0m% on \u001b[35m50\u001b[0m random-goal episodes\n",
      "Adjusted goal randomization success rate on unique different goals: \u001b[35m28.6\u001b[0m %\n",
      "Goal randomization mean episode length:\u001b[35m 106.46 \u001b[0m\n",
      "Goal positions with successes (unique): {(2, 4), (2, 7), (9, 9), (2, 3), (8, 9), (5, 6), (2, 2), (2, 5), (4, 7), (2, 8)}\n",
      "--- seed_763.zip ---\n",
      "Goal randomization success rate:\u001b[35m 32.0 \u001b[0m% on \u001b[35m50\u001b[0m random-goal episodes\n",
      "Adjusted goal randomization success rate on unique different goals: \u001b[35m34.3\u001b[0m %\n",
      "Goal randomization mean episode length:\u001b[35m 103.62 \u001b[0m\n",
      "Goal positions with successes (unique): {(8, 8), (2, 4), (2, 7), (4, 9), (2, 3), (8, 9), (7, 6), (8, 6), (2, 2), (2, 5), (6, 9), (2, 8)}\n",
      "--- seed_5672.zip ---\n",
      "Goal randomization success rate:\u001b[35m 46.0 \u001b[0m% on \u001b[35m50\u001b[0m random-goal episodes\n",
      "Adjusted goal randomization success rate on unique different goals: \u001b[35m48.6\u001b[0m %\n",
      "Goal randomization mean episode length:\u001b[35m 83.34 \u001b[0m\n",
      "Goal positions with successes (unique): {(8, 8), (2, 4), (2, 7), (4, 9), (8, 7), (5, 7), (2, 3), (4, 5), (8, 9), (7, 6), (5, 6), (8, 6), (2, 2), (2, 5), (6, 9), (4, 7), (2, 8)}\n",
      "--- seed_4744.zip ---\n",
      "Goal randomization success rate:\u001b[35m 20.0 \u001b[0m% on \u001b[35m50\u001b[0m random-goal episodes\n",
      "Adjusted goal randomization success rate on unique different goals: \u001b[35m17.1\u001b[0m %\n",
      "Goal randomization mean episode length:\u001b[35m 120.72 \u001b[0m\n",
      "Goal positions with successes (unique): {(4, 9), (5, 7), (2, 3), (2, 5), (6, 9), (2, 8)}\n",
      "--- seed_4267.zip ---\n",
      "Goal randomization success rate:\u001b[35m 18.0 \u001b[0m% on \u001b[35m50\u001b[0m random-goal episodes\n",
      "Adjusted goal randomization success rate on unique different goals: \u001b[35m20.0\u001b[0m %\n",
      "Goal randomization mean episode length:\u001b[35m 123.64 \u001b[0m\n",
      "Goal positions with successes (unique): {(2, 4), (2, 7), (4, 9), (2, 3), (2, 2), (2, 5), (2, 8)}\n"
     ]
    }
   ],
   "source": [
    "# Cell put in raw mode due to high computational cost\n",
    "# (output should always be identical due to random seed resetting)\n",
    "m1 = model_path_prefix + \"exp_001_goal_rnd_5/pixel_obs_8/a2c/a2c_entropy_6/seed_3377.zip\"\n",
    "m2 = model_path_prefix + \"exp_001_goal_rnd_5/pixel_obs_8/a2c/a2c_entropy_6/seed_763.zip\"\n",
    "m3 = model_path_prefix + \"exp_001_goal_rnd_5/pixel_obs_8/a2c/a2c_entropy_6/seed_5672.zip\"\n",
    "m4 = model_path_prefix + \"exp_001_goal_rnd_5/pixel_obs_8/a2c/a2c_entropy_6/seed_4744.zip\"\n",
    "m5 = model_path_prefix + \"exp_001_goal_rnd_5/pixel_obs_8/a2c/a2c_entropy_6/seed_4267.zip\"\n",
    "print(\"---\", m1.split(\"/\")[-1], \"---\")\n",
    "randomized_goal_stats(m1)\n",
    "print(\"---\", m2.split(\"/\")[-1], \"---\")\n",
    "randomized_goal_stats(m2)\n",
    "print(\"---\", m3.split(\"/\")[-1], \"---\")\n",
    "randomized_goal_stats(m3)\n",
    "print(\"---\", m4.split(\"/\")[-1], \"---\")\n",
    "randomized_goal_stats(m4)\n",
    "print(\"---\", m5.split(\"/\")[-1], \"---\")\n",
    "randomized_goal_stats(m5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 29.72\n"
     ]
    }
   ],
   "source": [
    "print(\"mean:\", sum([28.6, 34.3, 48.6, 17.1, 20])/5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Range of 20 - 48.6 % generalization capabilities"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "test_agent_on_environment(m3, custom_environment=make_env(goal_positions=[(8,8)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `time_penalty`\n",
    "\n",
    "*Environment Configuration:*\n",
    "\n",
    "```json\n",
    "    \"time_penalty\" : {\n",
    "        \"max_steps\" : 150,\n",
    "        \"slip_proba\" : 0,\n",
    "        \"wall_rebound\" : false,\n",
    "        \"spiky_active\" : false,\n",
    "        \"reward_spec\" : {\n",
    "            \"step_penalty\" : -0.1,\n",
    "            \"goal_reward\" : 1,\n",
    "            \"absorbing_states\" : false,\n",
    "            \"absorbing_reward_goal\" : 0,\n",
    "            \"absorbing_reward_lava\" : -1,\n",
    "            \"risky_tile_reward\" : 0,\n",
    "            \"lava_reward\" : -1\n",
    "        }\n",
    "    }\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A2C on `time_penalty`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Slipping Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `exp_slip_1`\n",
    "\n",
    "_Environment Configuration:_\n",
    "\n",
    "```json\n",
    "    \"exp_slip_1\" : {\n",
    "        \"max_steps\" : 150,\n",
    "        \"slip_proba\" : 0.05,\n",
    "        \"wall_rebound\" : false,\n",
    "        \"spiky_active\" : false,\n",
    "        \"reward_spec\" : {\n",
    "            \"step_penalty\" : 0,\n",
    "            \"goal_reward\" : 1,\n",
    "            \"absorbing_states\" : false,\n",
    "            \"absorbing_reward_goal\" : 0,\n",
    "            \"absorbing_reward_lava\" : -1,\n",
    "            \"risky_tile_reward\" : 0,\n",
    "            \"lava_reward\" : -1\n",
    "        }\n",
    "    }\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `exp_slip_2`\n",
    "\n",
    "_Environment Configuration:_\n",
    "\n",
    "```json\n",
    "    \"exp_slip_2\" : {\n",
    "        \"max_steps\" : 150,\n",
    "        \"slip_proba\" : 0.1,\n",
    "        \"wall_rebound\" : false,\n",
    "        \"spiky_active\" : false,\n",
    "        \"reward_spec\" : {\n",
    "            \"step_penalty\" : 0,\n",
    "            \"goal_reward\" : 1,\n",
    "            \"absorbing_states\" : false,\n",
    "            \"absorbing_reward_goal\" : 0,\n",
    "            \"absorbing_reward_lava\" : -1,\n",
    "            \"risky_tile_reward\" : 0,\n",
    "            \"lava_reward\" : -1\n",
    "        }\n",
    "    }\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `exp_slip_3`\n",
    "\n",
    "_Environment Configuration:_\n",
    "\n",
    "```json\n",
    "    \"exp_slip_3\" : {\n",
    "        \"max_steps\" : 150,\n",
    "        \"slip_proba\" : 0.15,\n",
    "        \"wall_rebound\" : false,\n",
    "        \"spiky_active\" : false,\n",
    "        \"reward_spec\" : {\n",
    "            \"step_penalty\" : 0,\n",
    "            \"goal_reward\" : 1,\n",
    "            \"absorbing_states\" : false,\n",
    "            \"absorbing_reward_goal\" : 0,\n",
    "            \"absorbing_reward_lava\" : -1,\n",
    "            \"risky_tile_reward\" : 0,\n",
    "            \"lava_reward\" : -1\n",
    "        }\n",
    "    }\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 ('sb3')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0d41f5c1acf177a218bc0139b8f3c17ccec1007898cc8a9ab8dc8cb303ffab48"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
