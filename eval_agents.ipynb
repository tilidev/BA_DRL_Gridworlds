{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating trained agents\n",
    "\n",
    "This Notebook will be used to visualize & analyze various trained agents on RiskyPath environment. Analysis will especially comprise observing the agent's behaviour in the environment it was trained for but also different versions of the environment (distributional shift analysis)\n",
    "\n",
    "To observe deterministic agent behaviour, slipping/collision probabilities might be set to zero. In this case, the agent might not be tested on the exact same environment configuration it was trained on. In every case, this will be marked in the corresponding cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "\n",
    "import gym\n",
    "import gym_minigrid\n",
    "from gym_minigrid.minigrid import Goal, Floor, Lava, Wall, SpikyTile\n",
    "from gym_minigrid.envs import RiskyPathEnv\n",
    "from gym_minigrid.wrappers import RGBImgObsWrapper, ImgObsWrapper, TensorObsWrapper\n",
    "from special_wrappers import RandomizeGoalWrapper\n",
    "\n",
    "from experiment_config import GridworldExperiment\n",
    "import torch as th\n",
    "import stable_baselines3\n",
    "from stable_baselines3.dqn import DQN\n",
    "from stable_baselines3.a2c import A2C\n",
    "from stable_baselines3.common.utils import obs_as_tensor\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE Save the prefix for the logs & models folder here for compatibility across different systems\n",
    "model_path_prefix = \"/Users/tilioschulze/Library/CloudStorage/OneDrive-Personal/Studium/Bachelorarbeit/experiment_models/saved_models/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilities\n",
    "\n",
    "Definition of functions to use for quick analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "sinfo = \"\\33[32mINFO:\\33[0m\"\n",
    "\n",
    "def model_env_from_path(agent_path: str, no_slip: bool = True, no_rebound: bool = True):\n",
    "    # Extract model from path (a2c or dqn?)\n",
    "    if \"/dqn/\" in agent_path:\n",
    "        model_class = DQN\n",
    "    elif \"/a2c/\" in agent_path:\n",
    "        model_class = A2C\n",
    "\n",
    "    model = model_class.load(agent_path)\n",
    "\n",
    "    # Create environment given information in function input\n",
    "    path_keys = agent_path.split(\"saved_models/\")[1].split(\"/\")\n",
    "    env_name = path_keys[0]\n",
    "    observation_type = path_keys[1]\n",
    "\n",
    "    render_size = 8\n",
    "    rgb = False\n",
    "    if \"pixel_obs_\" in agent_path:\n",
    "        render_size = int(path_keys[1].split(\"_\")[-1])\n",
    "        rgb = True\n",
    "\n",
    "    env_info = \"\"\n",
    "    with open('env_config.json', 'r') as f:\n",
    "        env_kwargs = json.load(f)[env_name]\n",
    "    if 'goal_rnd' in env_kwargs:\n",
    "        env_kwargs.pop('goal_rnd')\n",
    "    if no_slip and env_kwargs['slip_proba'] != 0:\n",
    "        env_kwargs.pop('slip_proba')\n",
    "        env_info += \"slipping probability removed; \"\n",
    "    if no_rebound and env_kwargs['wall_rebound']:\n",
    "        env_kwargs.pop('wall_rebound')\n",
    "        env_info += \"wall rebound deactivated\"\n",
    "    if len(env_info) != 0:\n",
    "        print(\"\\33[32mINFO:\\33[0m\", env_info)\n",
    "\n",
    "    env = gym.make(\n",
    "        \"MiniGrid-RiskyPath-v0\",\n",
    "        **env_kwargs\n",
    "    )\n",
    "\n",
    "    return model, env, rgb, render_size\n",
    "\n",
    "def test_agent_on_environment(\n",
    "    agent_path: str,\n",
    "    num_episodes: int = 1,\n",
    "    render_time: float = 0.2,\n",
    "    custom_environment: gym.Env = None,\n",
    "    predict_deterministic: bool = True,\n",
    "    accelerate_viz: bool = True\n",
    "):\n",
    "    \"\"\"Render agent interaction with the environment in an interactive matplotlib window. Useful to make videos of agent behaviour or analyzing trajectories. Slipping and wall rebound is turned off in order to observe the agent's intended behaviour. When passing a custom environment, no checking for stationary state distribution and deterministic transitions is performed.\n",
    "\n",
    "    Args:\n",
    "        agent_path (str): Model location. Folder path must conform to experiment structure\n",
    "        num_episodes (int, optional): number of episodes to render\n",
    "        render_time (float, optional): render time for one time step in seconds\n",
    "        custom_environment (gym.Env, optional): None by default\n",
    "        predict_deterministic (bool, optional): Make deterministic mode predictions\n",
    "        accelerate_viz (bool, optional): Will accelerate rendering when agent takes too long to solve environment\n",
    "    \"\"\"\n",
    "    model, env, rgb_on, render_size = model_env_from_path(agent_path)\n",
    "\n",
    "    if custom_environment is not None:\n",
    "        env = custom_environment\n",
    "    \n",
    "    if rgb_on:\n",
    "        env = RGBImgObsWrapper(env, tile_size=render_size)\n",
    "        env = ImgObsWrapper(env)\n",
    "    else:\n",
    "        env = TensorObsWrapper(env)\n",
    "    \n",
    "    # Execute episodes and render agent\n",
    "        # TODO print reward, action [number, (himmelsrichtung)] etc.\n",
    "    for i in range(num_episodes):\n",
    "\n",
    "        print(f\"Starting episode {i+1}\")\n",
    "        total_reward = 0\n",
    "        needed_timesteps = 0\n",
    "\n",
    "        obs = env.reset()\n",
    "        done = False\n",
    "        env.render(tile_size=render_size)\n",
    "        time.sleep(render_time)\n",
    "\n",
    "        while not done:\n",
    "            action, _ = model.predict(obs, deterministic=predict_deterministic)\n",
    "            obs, reward, done, info = env.step(action)\n",
    "            env.render(tile_size=render_size)\n",
    "            total_reward += reward\n",
    "            needed_timesteps += 1\n",
    "            if needed_timesteps > 25:\n",
    "                render_time = 0.05\n",
    "            time.sleep(render_time)\n",
    "        \n",
    "        print(f\"Episode ended after {needed_timesteps} time steps.\")\n",
    "        out = f\"Total reward: {total_reward}\"\n",
    "        print(out)\n",
    "        print(\"-\"*len(out))\n",
    "    \n",
    "    %matplotlib\n",
    "\n",
    "def make_env(\n",
    "    **kwargs\n",
    "):\n",
    "    env = gym.make(\n",
    "        \"MiniGrid-RiskyPath-v0\",\n",
    "        **kwargs\n",
    "    )\n",
    "    return env\n",
    "\n",
    "def compute_q_values(model_policy, obs):\n",
    "    \"\"\"Compute q-values from a DQN model given a certain observation.\n",
    "\n",
    "    Args:\n",
    "        model_policy: The DQN model's policy\n",
    "        obs: The environmental observation for which q-values should be computed\n",
    "    \"\"\"\n",
    "    # Code adapted from this stackoverflow post\n",
    "    # https://stackoverflow.com/questions/73239501/how-to-get-the-q-values-in-dqn-in-stable-baseline-3/73242315#73242315?newreg=d2762c51b8bc44778cde16b43499a6d5\n",
    "    observation = obs.reshape((-1,) + model_policy.observation_space.shape)\n",
    "    observation = obs_as_tensor(observation, \"cpu\")\n",
    "    with th.no_grad():\n",
    "        q_values = model_policy.q_net(observation)\n",
    "    return q_values\n",
    "\n",
    "def visualize_policy(\n",
    "    path: str,\n",
    "    custom_env = None\n",
    "):\n",
    "    \"\"\"Visualize the model policy on the given environment specification. The environmental state distribution is assumed to stationary. Goal randomization is explicitly not applied. A custom environment can be passed and policy will be applied on it. This method does not check if the state distribution is stationary. Returns a colored string representation to print to the console.\n",
    "\n",
    "    Args:\n",
    "        path (str): The saved models location\n",
    "        custom_env (RiskyPathEnv): A custom environment\n",
    "    \"\"\"    \n",
    "    model, env, rgb_on, render_size = model_env_from_path(path)\n",
    "    if custom_env is not None:\n",
    "        env = custom_env\n",
    "    \n",
    "    # No wrapping is needed\n",
    "    env.reset()\n",
    "\n",
    "    grid = env.grid\n",
    "    visual_policy = \"\"\n",
    "\n",
    "    ansi_color = lambda code, text:  f\"\\33[{code}m{text}\\33[0m\"\n",
    "\n",
    "    for i in range(grid.width):\n",
    "        visual_policy += \" \" + str(i) + \"  \"\n",
    "        if i == grid.width - 1:\n",
    "            visual_policy += \"\\n\"\n",
    "\n",
    "    for y in range(grid.height):\n",
    "        for x in range(grid.width):\n",
    "            tile = grid.get(x, y)\n",
    "            \n",
    "            if tile is None or isinstance(tile, Floor) or isinstance(tile, SpikyTile):\n",
    "                # get model action and map to <, >, ^, v strings\n",
    "                # NOTE setting a variable only works on unwrapped env as gym automatically wraps the environment with orderenforcing wrapper and wrappers do not implement a __setattr__ method but a __getattr__\n",
    "                env.unwrapped.agent_pos = (x, y)\n",
    "                if rgb_on:\n",
    "                    obs = env.render(\n",
    "                        mode=\"rgb_array\",\n",
    "                        highlight=False,\n",
    "                        tile_size=render_size\n",
    "                    )\n",
    "                else:\n",
    "                    obs = env.tensor_obs()\n",
    "\n",
    "                dir_mapping = {0 : \"<\", 1 : \"^\", 2 : \">\", 3 : \"v\"}\n",
    "                action = int(model.predict(obs, deterministic=True)[0])\n",
    "                dir_str = dir_mapping[action]\n",
    "\n",
    "                visual_policy += f\"[{dir_str}] \"\n",
    "            elif isinstance(tile, Wall):\n",
    "                w = ansi_color(36, \"#\")\n",
    "                visual_policy += f\"[{w}] \"\n",
    "            elif isinstance(tile, Lava):\n",
    "                l = ansi_color(41, \"~\")\n",
    "                visual_policy += f\"[{l}] \"\n",
    "            elif isinstance(tile, Goal):\n",
    "                g = ansi_color(42, \"x\")\n",
    "                visual_policy += f\"[{g}] \"\n",
    "            \n",
    "            if x == grid.width - 1: \n",
    "                visual_policy += f\" {y} \\n\"\n",
    "            \n",
    "    return visual_policy\n",
    "\n",
    "def load_model_params(\n",
    "    path: str\n",
    "):\n",
    "    \"\"\"Return model policy and additional information\n",
    "\n",
    "    Args:\n",
    "        path (str): The path to the saved model\n",
    "\n",
    "    Returns:\n",
    "        tuple: policy, policy_class, policy_kwargs\n",
    "    \"\"\"\n",
    "    # Extract model from path (a2c or dqn?)\n",
    "    if \"/dqn/\" in path:\n",
    "        model_class = DQN\n",
    "    elif \"/a2c/\" in path:\n",
    "        model_class = A2C\n",
    "\n",
    "    model = model_class.load(path)\n",
    "    return model.policy, model.policy_class, model.policy_kwargs\n",
    "\n",
    "def dqn_params(path):\n",
    "    if \"/dqn/\" in path:\n",
    "        model_class = DQN\n",
    "    model = model_class.load(path)\n",
    "    return model.get_parameters()\n",
    "\n",
    "def randomized_goal_stats(path: str, episodes: int = 50):\n",
    "    \"\"\"Test the agent statistically on the training environment specification but with randomized goal tile placement. Other environmental factors are taken from the env_config key except for slipping and wall rebound which is turned off in order to truly analyze the agent's capabilities.\n",
    "\n",
    "    Args:\n",
    "        path (str): Location of the saved model. Must conform to predefined folder structure.\n",
    "        episodes (int, optional): Number random-goal episodes\n",
    "    \"\"\"        \n",
    "    model, env, rgb_on, render_size = model_env_from_path(path)\n",
    "    count_successes = 0\n",
    "    if rgb_on:\n",
    "        env = RGBImgObsWrapper(env, render_size)\n",
    "        env = ImgObsWrapper(env)\n",
    "    else:\n",
    "        env = TensorObsWrapper(env)\n",
    "\n",
    "    env = RandomizeGoalWrapper(env, randomization=1)\n",
    "\n",
    "    episode_lengths = []\n",
    "    for _ in range(episodes):\n",
    "        obs = env.reset()\n",
    "        done = False\n",
    "        step = 0\n",
    "        while not done:\n",
    "            action, _ = model.predict(obs, deterministic=True)\n",
    "            obs, _, done, info = env.step(action)\n",
    "            step += 1\n",
    "        if info['is_success']: count_successes += 1\n",
    "        episode_lengths.append(step)\n",
    "    \n",
    "    print(\"Goal randomization success rate:\\33[35m\", round(count_successes/episodes, 1), f\"\\33[0m% on \\33[35m{episodes}\\33[0m random-goal episodes\")\n",
    "    print(\"Goal randomization mean episode length:\\33[35m\", np.mean(episode_lengths), \"\\33[0m\")\n",
    "\n",
    "def execute_episode(path: str):\n",
    "    \"\"\"Execute a test episode with the specified saved model on the training environment configuration. Slipping and wall rebound is deactivated. Prints episode summary to stdout.\n",
    "\n",
    "    Args:\n",
    "        path (str): Location of the saved model. Must conform to specified structure (see experiment_config.py)\n",
    "    \"\"\" \n",
    "    model, env, rgb, render_size = model_env_from_path(path)\n",
    "    if rgb:\n",
    "        env = RGBImgObsWrapper(env, tile_size=render_size)\n",
    "        env = ImgObsWrapper(env)\n",
    "    else:\n",
    "        env = TensorObsWrapper(env)\n",
    "\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    count_steps = 0\n",
    "    cumulative_reward = 0\n",
    "    while not done:\n",
    "        action, _ = model.predict(obs, deterministic=True)\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        cumulative_reward += reward\n",
    "        count_steps += 1\n",
    "    print(f\"Episode summary -> success: \\33[35m{info['is_success']}\\33[0m, cumulative reward: \\33[35m{cumulative_reward}\\33[0m, number of steps: \\33[35m{count_steps}\\33[0m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining standard test suite\n",
    "def test_suite_model(path: str):\n",
    "    print(sinfo, \"Beginning execution of test suite.\")\n",
    "    print(f\"Path: \\33[3m{path[len(model_path_prefix):]}\\33[0m\")\n",
    "\n",
    "    # test agent on deterministic (!) environment\n",
    "    print(\"\\n\\33[4mAgent success on \\33[1mdeterministic\\33[0;4m training environment:\\33[0m\")\n",
    "    execute_episode(path)\n",
    "\n",
    "    # visualized policy on original environment\n",
    "    print(\"\\n\\33[4mPolicy visualization on training environment:\\33[0m\")\n",
    "    print(visualize_policy(path))\n",
    "    \n",
    "    # summary statistics on randomized goal locations\n",
    "    print(\"\\33[4mTesting Goal generalization capabilities:\\33[0m\")\n",
    "    randomized_goal_stats(path)\n",
    "\n",
    "    print(sinfo, \"Test suite execution ended.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_lava_positions = []\n",
    "for y in range(1, 11 - 1):\n",
    "    base_lava_positions.append((1, y))\n",
    "for y in range(11 - 3, 11 - 8, -1):\n",
    "    base_lava_positions.append((3, y))\n",
    "base_lava_positions.extend([(6, 11 - 5), (6, 11 - 6)])\n",
    "base_lava_positions.remove((1,3)) # remove location of original goal position\n",
    "\n",
    "upper_right_goal_env = lambda: make_env(goal_positions=[(9, 1)])\n",
    "alt_upper_right_goal_env = lambda: make_env(goal_positions=[(9, 1)], lava_positions=base_lava_positions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ignore \"memory not enough\" warnings concerning replay buffer\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', module=\"stable_baselines3.common.buffers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: <object object at 0x14eca0e50>\n"
     ]
    }
   ],
   "source": [
    "%matplotlib\n",
    "# Force matplotlib to render outside of notebook (Don't use 'inline' backend)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## exp_001\n",
    "\n",
    "_Environment configuration:_\n",
    "```json\n",
    "    \"exp_001\" : {\n",
    "        \"max_steps\" : 150,\n",
    "        \"slip_proba\" : 0,\n",
    "        \"wall_rebound\" : false,\n",
    "        \"spiky_active\" : false,\n",
    "        \"reward_spec\" : {\n",
    "            \"step_penalty\" : 0,\n",
    "            \"goal_reward\" : 1,\n",
    "            \"absorbing_states\" : false,\n",
    "            \"absorbing_reward_goal\" : 0,\n",
    "            \"absorbing_reward_lava\" : -1,\n",
    "            \"risky_tile_reward\" : 0,\n",
    "            \"lava_reward\" : -1\n",
    "        }\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's load one of the successful trained agents on the `stable-baselines3` DQN defaults. It was trained on tensor observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_001_path = model_path_prefix + \"exp_001/tensor_obs/dqn/algo_default/seed_763.zip\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO:\u001b[0m \n",
      "Starting episode 1\n",
      "Episode ended after 7 time steps.\n",
      "Total reward: 1\n",
      "---------------\n",
      "Using matplotlib backend: MacOSX\n"
     ]
    }
   ],
   "source": [
    "test_agent_on_environment(exp_001_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agent walks to the goal tile. Let's visualize it's policy on this version of the environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO:\u001b[0m Beginning execution of test suite.\n",
      "Path: \u001b[3mexp_001/tensor_obs/dqn/algo_default/seed_763.zip\u001b[0m\n",
      "\n",
      "\u001b[4mAgent success on \u001b[1mdeterministic\u001b[0;4m training environment:\u001b[0m\n",
      "Episode summary -> success: \u001b[35mTrue\u001b[0m, cumulative reward: \u001b[35m1\u001b[0m, number of steps: \u001b[35m7\u001b[0m\n",
      "\n",
      "\u001b[4mPolicy visualization on training environment:\u001b[0m\n",
      " 0   1   2   3   4   5   6   7   8   9   10  \n",
      "[\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m]  0 \n",
      "[\u001b[36m#\u001b[0m] [\u001b[41m~\u001b[0m] [v] [<] [<] [<] [v] [<] [<] [<] [\u001b[36m#\u001b[0m]  1 \n",
      "[\u001b[36m#\u001b[0m] [\u001b[41m~\u001b[0m] [v] [<] [<] [v] [v] [v] [<] [<] [\u001b[36m#\u001b[0m]  2 \n",
      "[\u001b[36m#\u001b[0m] [\u001b[42mx\u001b[0m] [<] [<] [<] [<] [<] [<] [<] [<] [\u001b[36m#\u001b[0m]  3 \n",
      "[\u001b[36m#\u001b[0m] [\u001b[41m~\u001b[0m] [^] [\u001b[41m~\u001b[0m] [^] [<] [<] [^] [^] [<] [\u001b[36m#\u001b[0m]  4 \n",
      "[\u001b[36m#\u001b[0m] [\u001b[41m~\u001b[0m] [^] [\u001b[41m~\u001b[0m] [^] [<] [\u001b[41m~\u001b[0m] [^] [<] [>] [\u001b[36m#\u001b[0m]  5 \n",
      "[\u001b[36m#\u001b[0m] [\u001b[41m~\u001b[0m] [^] [\u001b[41m~\u001b[0m] [^] [<] [\u001b[41m~\u001b[0m] [^] [^] [>] [\u001b[36m#\u001b[0m]  6 \n",
      "[\u001b[36m#\u001b[0m] [\u001b[41m~\u001b[0m] [^] [\u001b[41m~\u001b[0m] [^] [<] [<] [<] [<] [<] [\u001b[36m#\u001b[0m]  7 \n",
      "[\u001b[36m#\u001b[0m] [\u001b[41m~\u001b[0m] [^] [\u001b[41m~\u001b[0m] [v] [v] [<] [<] [<] [<] [\u001b[36m#\u001b[0m]  8 \n",
      "[\u001b[36m#\u001b[0m] [\u001b[41m~\u001b[0m] [^] [<] [<] [<] [<] [<] [<] [<] [\u001b[36m#\u001b[0m]  9 \n",
      "[\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m]  10 \n",
      "\n",
      "\u001b[4mTesting Goal generalization capabilities:\u001b[0m\n",
      "Goal randomization success rate:\u001b[35m 0.0 \u001b[0m% on \u001b[35m50\u001b[0m random-goal episodes\n",
      "Goal randomization mean episode length:\u001b[35m 147.14 \u001b[0m\n",
      "\u001b[32mINFO:\u001b[0m Test suite execution ended.\n"
     ]
    }
   ],
   "source": [
    "test_suite_model(exp_001_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation of output:**\n",
    "\n",
    "- `[~]` is lava\n",
    "- `[#]` is a wall\n",
    "- `[x]` is the goal tile\n",
    "- `<,^,>,v` are the directions the agent would take from that cell\n",
    "\n",
    "We see that the agent would walk to the goal tile from almost each position, with some exceptions: (9,5), (9,6). In no case would the agent walk in one of the lava tiles.\n",
    "The agent has fulfilled its task of walking to the goal tile and it can even do that from most other grid positions.\n",
    "However, let's test the agent on the same environment but move the goal position somewhere else."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting episode 1\n",
      "Episode ended after 150 time steps.\n",
      "Total reward: 0\n",
      "---------------\n",
      "Using matplotlib backend: MacOSX\n"
     ]
    }
   ],
   "source": [
    "test_agent_on_environment(\n",
    "    exp_001_path,\n",
    "    num_episodes=1,\n",
    "    custom_environment=upper_right_goal_env()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0   1   2   3   4   5   6   7   8   9   10  \n",
      "[\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m]  0 \n",
      "[\u001b[36m#\u001b[0m] [\u001b[41m~\u001b[0m] [v] [<] [<] [<] [<] [<] [>] [\u001b[42mx\u001b[0m] [\u001b[36m#\u001b[0m]  1 \n",
      "[\u001b[36m#\u001b[0m] [\u001b[41m~\u001b[0m] [v] [<] [<] [<] [<] [<] [<] [<] [\u001b[36m#\u001b[0m]  2 \n",
      "[\u001b[36m#\u001b[0m] [\u001b[41m~\u001b[0m] [>] [<] [<] [<] [<] [<] [<] [<] [\u001b[36m#\u001b[0m]  3 \n",
      "[\u001b[36m#\u001b[0m] [\u001b[41m~\u001b[0m] [v] [\u001b[41m~\u001b[0m] [^] [<] [<] [<] [<] [>] [\u001b[36m#\u001b[0m]  4 \n",
      "[\u001b[36m#\u001b[0m] [\u001b[41m~\u001b[0m] [v] [\u001b[41m~\u001b[0m] [^] [<] [\u001b[41m~\u001b[0m] [>] [>] [>] [\u001b[36m#\u001b[0m]  5 \n",
      "[\u001b[36m#\u001b[0m] [\u001b[41m~\u001b[0m] [v] [\u001b[41m~\u001b[0m] [^] [<] [\u001b[41m~\u001b[0m] [>] [>] [>] [\u001b[36m#\u001b[0m]  6 \n",
      "[\u001b[36m#\u001b[0m] [\u001b[41m~\u001b[0m] [v] [\u001b[41m~\u001b[0m] [>] [<] [>] [>] [>] [>] [\u001b[36m#\u001b[0m]  7 \n",
      "[\u001b[36m#\u001b[0m] [\u001b[41m~\u001b[0m] [^] [\u001b[41m~\u001b[0m] [v] [<] [>] [>] [>] [>] [\u001b[36m#\u001b[0m]  8 \n",
      "[\u001b[36m#\u001b[0m] [\u001b[41m~\u001b[0m] [>] [<] [<] [<] [<] [>] [>] [>] [\u001b[36m#\u001b[0m]  9 \n",
      "[\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m]  10 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(visualize_policy(exp_001_path, upper_right_goal_env()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The agent oscillates between the starting position (2,9) and the adjacent tile to the right. It seems confused about the changed environment. Two factors seem to come into play: The goal tile was changed to (9,9) and the old goal tile was replaced with a lava tile. The agent might have recognized that its previous strategy might no longer be safe. Given the policy visualization for this environment verions, this is further evidenced by the fact that the agent still tries to avoid all lava tiles, even the new lava tile at (1,3).\n",
    "Let's see how the agent behaviour changes when the original goal tile is turned to floor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting episode 1\n",
      "Episode ended after 150 time steps.\n",
      "Total reward: 0\n",
      "---------------\n",
      "Using matplotlib backend: MacOSX\n"
     ]
    }
   ],
   "source": [
    "test_agent_on_environment(exp_001_path, custom_environment=alt_upper_right_goal_env())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0   1   2   3   4   5   6   7   8   9   10  \n",
      "[\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m]  0 \n",
      "[\u001b[36m#\u001b[0m] [\u001b[41m~\u001b[0m] [v] [<] [<] [<] [<] [<] [<] [\u001b[42mx\u001b[0m] [\u001b[36m#\u001b[0m]  1 \n",
      "[\u001b[36m#\u001b[0m] [\u001b[41m~\u001b[0m] [v] [<] [<] [<] [<] [<] [<] [<] [\u001b[36m#\u001b[0m]  2 \n",
      "[\u001b[36m#\u001b[0m] [v] [<] [<] [<] [<] [<] [<] [<] [<] [\u001b[36m#\u001b[0m]  3 \n",
      "[\u001b[36m#\u001b[0m] [\u001b[41m~\u001b[0m] [^] [\u001b[41m~\u001b[0m] [^] [<] [<] [<] [<] [<] [\u001b[36m#\u001b[0m]  4 \n",
      "[\u001b[36m#\u001b[0m] [\u001b[41m~\u001b[0m] [^] [\u001b[41m~\u001b[0m] [^] [<] [\u001b[41m~\u001b[0m] [>] [<] [>] [\u001b[36m#\u001b[0m]  5 \n",
      "[\u001b[36m#\u001b[0m] [\u001b[41m~\u001b[0m] [^] [\u001b[41m~\u001b[0m] [^] [<] [\u001b[41m~\u001b[0m] [>] [>] [>] [\u001b[36m#\u001b[0m]  6 \n",
      "[\u001b[36m#\u001b[0m] [\u001b[41m~\u001b[0m] [^] [\u001b[41m~\u001b[0m] [^] [<] [>] [<] [<] [>] [\u001b[36m#\u001b[0m]  7 \n",
      "[\u001b[36m#\u001b[0m] [\u001b[41m~\u001b[0m] [^] [\u001b[41m~\u001b[0m] [v] [<] [<] [<] [>] [>] [\u001b[36m#\u001b[0m]  8 \n",
      "[\u001b[36m#\u001b[0m] [\u001b[41m~\u001b[0m] [>] [<] [<] [<] [<] [<] [>] [>] [\u001b[36m#\u001b[0m]  9 \n",
      "[\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m] [\u001b[36m#\u001b[0m]  10 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(visualize_policy(exp_001_path, custom_env=alt_upper_right_goal_env()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the agent still oscillates at the starting position and its right neighbour, however, its policy would lead the agent to walk to the original goal position when placed in most other cells (and then terminating the episode by walking in the lava tile below). Note that no policy-induced trajectory would end up in the actual goal tile given this environment.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Next, I'd like to see if the agent is able to avoid _newly placed_ lava tiles "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(visualize_policy(exp_001_path, custom_env=))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Summary:**\n",
    "\n",
    "- During training, the agent learned to walk to the goal tile successfully\n",
    "- However, the agent is not fully able to "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting episode 1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "Episode ended after 9 time steps.\n",
      "Total reward: 1\n",
      "---------------\n",
      "Using matplotlib backend: MacOSX\n",
      "Starting episode 1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "Episode ended after 8 time steps.\n",
      "Total reward: 1\n",
      "---------------\n",
      "Using matplotlib backend: MacOSX\n",
      "Starting episode 1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "Episode ended after 12 time steps.\n",
      "Total reward: 1\n",
      "---------------\n",
      "Using matplotlib backend: MacOSX\n",
      "Starting episode 1\n",
      "3\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "Episode ended after 10 time steps.\n",
      "Total reward: 1\n",
      "---------------\n",
      "Using matplotlib backend: MacOSX\n"
     ]
    }
   ],
   "source": [
    "for pos in [(7,6), (3,9), (9,7), (4,8)]:\n",
    "    test_agent_on_environment(\n",
    "        exp_001_path,\n",
    "        num_episodes=1,\n",
    "        custom_environment=make_env(\n",
    "            agent_start_pos=pos\n",
    "        ),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The agent successfully navigates the environment when beginning at another position. It quickly finds the goal tile and mostly doesn't take any detours.\n",
    "Interestingly, when the agent is placed on position (4,8) it first goes down and to the left instead of taking the quicker path upwards. Considering that the reward model of `exp_001` does not incentivize the agent to find the shortest path (no time penalty), this is not especially surprising. Still, this leads to the hypothesis that the agent found that going downwards from this position would lead to more reward than going up. (Or maybe due to the update rule in Q-Learning? --> # TODO investigate this)\n",
    "\n",
    "What happens when lava tiles are placed in the agent's way?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt_lava_positions = []\n",
    "for y in range(1, 11 - 1):\n",
    "    alt_lava_positions.append((1, y))\n",
    "for y in range(11 - 3, 11 - 8, -1):\n",
    "    alt_lava_positions.append((3, y))\n",
    "alt_lava_positions.extend([(6, 11 - 5), (6, 11 - 6)])\n",
    "alt_lava_positions.append((2,8))\n",
    "\n",
    "test_agent_on_environment(\n",
    "    exp_001_path,\n",
    "    num_episodes=1,\n",
    "    custom_environment=make_env(\n",
    "        lava_positions=alt_lava_positions\n",
    "    )\n",
    ")\n",
    "\n",
    "alt_lava_positions.append((3,3))\n",
    "test_agent_on_environment(\n",
    "    exp_001_path,\n",
    "    num_episodes=1,\n",
    "    custom_environment=make_env(\n",
    "        lava_positions=alt_lava_positions,\n",
    "        agent_start_pos=(4,7)\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that the agent is not able to recognize lava tiles. It always goes straight to the goal location and only evades the lava tile positions that it already knows from training. It seems the agent has not learned the causation between lava and negative reward but instead learned the correlation between the positions (of lava tiles) in the gridworld and the negative reward. This would also explain why changing the starting position does not confuse the agent in searching the goal tile (when it is at the original position). During training, only one part of the observation tensor is constantly changing, namely the agent's position.\n",
    "**INTERESTING:** Train an agent on a self-shifting environment (e.g. change goal position every five episodes and change lava tile positions every 5 episodes)\n",
    "\n",
    "This leads to the following question: \n",
    "Does the agent find the goal when it is placed somewhere else?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_agent_on_environment(\n",
    "    exp_001_path,\n",
    "    num_episodes=1,\n",
    "    custom_environment=make_env(\n",
    "        max_steps=25, # Changed limit because agent is caught in a loop\n",
    "        goal_positions=[(2,2)]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is clearly visible that the agent is not able to understand how to get to the goal tile when placed somewhere else. It gets caught in a loop and oscillated from left to right (Maybe an analysis would be interesting in which the q-net's output would be shown for each possible state).\n",
    "The hypothesis is thus strengthened that the agent only effectively learns cause and effect when the environment dynamics change (e.g. lava and goal placement). Currently, the agent is only able "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load an a2c model that was very succesful during training. The next observation is an interesting one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_lava = []\n",
    "for y in range(1, 11 - 1):\n",
    "    orig_lava.append((1, y))\n",
    "for y in range(11 - 3, 11 - 8, -1):\n",
    "    orig_lava.append((3, y))\n",
    "orig_lava.remove((1,3))\n",
    "\n",
    "a2c_low_entropy_model = model_path_prefix + \"exp_001/tensor_obs/a2c/a2c_entropy_6/seed_4267.zip\"\n",
    "\n",
    "test_agent_on_environment(\n",
    "    a2c_low_entropy_model,\n",
    "    num_episodes=1,\n",
    "    custom_environment=make_env(\n",
    "        max_steps=25,\n",
    "        goal_positions=[(3,3)],\n",
    "        lava_positions=orig_lava\n",
    "    ),\n",
    "    render_time=0.4\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Distributional Shift, Goal Misgeneralization:**\n",
    "\n",
    "The agent does not understand that the important tile is the goal tile. It still navigates to the position in which it recieved positive rewards during training. Once the state distribution shifts, the agent is not able to apply the learned skills to a simple alteration of the environment. This shows that the state representation during training is not truly sufficient if we want the agent to be able to generalize knowledge.\n",
    "\n",
    "- [ ] Train agent on a self-shifting environment with different goal positions. Use algorithmic settings/parametrizations that worked best during training\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test on self-shifting environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_agent_on_environment(\n",
    "    a2c_low_entropy_model,\n",
    "    num_episodes=5,\n",
    "    render_time=0.4,\n",
    "    custom_environment=RandomizeGoalWrapper(make_env(), randomization=0.5)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The agent only reaches the goal tile when it is placed in on the training location! The agent has not learned to walk in the goal tile, it has learned to walk to the location where it recieved rewards during training. During training, this is a **perfect proxy of the intended goal**, which is to walk on the goal tile! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Goal Randomization Agent:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathlala = \"/Users/tilioschulze/Desktop/Bachelorarbeit/Code/Experiments/saved_models/exp_001_goal_rnd_2/tensor_obs/dqn/dqn_low_eps/seed_4744_best_model/best_model.zip\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_agent_on_environment(pathlala, num_episodes=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solves environment perfectly. What will happen when distributional shift is applied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_agent_on_environment(\n",
    "    pathlala,\n",
    "    num_episodes=6,\n",
    "    custom_environment=RandomizeGoalWrapper(make_env(), randomization=1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p, _, _ = load_model_params(exp_001_path)\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO create observation here\n",
    "env = gym.make(\"MiniGrid-RiskyPath-v0\")\n",
    "env = TensorObsWrapper(env)\n",
    "obs = env.reset()\n",
    "\n",
    "from stable_baselines3.common.utils import obs_as_tensor\n",
    "import torch as th\n",
    "\n",
    "observation = obs.reshape((-1,) + p.observation_space.shape)\n",
    "observation = obs_as_tensor(observation, \"cpu\")\n",
    "with th.no_grad():\n",
    "    q_values = p.q_net(observation)\n",
    "print(q_values)\n",
    "\n",
    "obs, _, _, _ = env.step(2)\n",
    "\n",
    "observation = obs.reshape((-1,) + p.observation_space.shape)\n",
    "observation = obs_as_tensor(observation, \"cpu\")\n",
    "with th.no_grad():\n",
    "    q_values = p.q_net(observation)\n",
    "print(q_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we look at the q-values, we can see that from the starting point, the agent prefers going right. After landing on this tile, the maximal q-value corresponds to moving to the left tile, which catches the agent in a loop!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A2C low entropy model on exp_001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "npath = model_path_prefix + \"exp_001/pixel_obs_8/a2c/a2c_entropy_6/seed_3377.zip\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_agent_on_environment(\n",
    "    npath\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This training-wise succesful a2c agent is also not able to generalize, when confronted with new lava tile positions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A2C entropy_6 on exp_001 (pixel_obs_8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_agent_on_environment(\n",
    "    model_path_prefix + \"exp_hard_001/pixel_obs_8/a2c/a2c_entropy_6/seed_4267.zip\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Success! The agent learns to walk around the spiky tiles and lava to maximize rewards.\n",
    "However, this is only one seed out of 5. The other 4 seeds failed to find such a solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt_lava_positions = []\n",
    "for y in range(1, 11 - 1):\n",
    "    alt_lava_positions.append((1, y))\n",
    "for y in range(11 - 3, 11 - 8, -1):\n",
    "    alt_lava_positions.append((3, y))\n",
    "alt_lava_positions.extend([(6, 11 - 5), (6, 11 - 6)])\n",
    "\n",
    "spiky_positions = []\n",
    "\n",
    "test_agent_on_environment(\n",
    "    model_path_prefix + \"exp_hard_001/pixel_obs_8/a2c/a2c_entropy_6/seed_4267.zip\",\n",
    "    custom_environment=make_env(\n",
    "        lava_positions=alt_lava_positions\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting --> Removing spiky tiles confuses the agent at the last step. Maybe it expects to walk on a spiky tile when finishing the Umweg, but does not recognize the situation.\n",
    "What happens when moving the goal tile?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_agent_on_environment(\n",
    "    model_path_prefix + \"exp_hard_001/pixel_obs_8/a2c/a2c_entropy_6/seed_4744.zip\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `time_penalty`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A2C on `time_penalty`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pen_a2c = model_path_prefix + \"time_penalty/tensor_obs/a2c/algo_default/seed_763.zip\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_agent_on_environment(\n",
    "    pen_a2c\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `exp_hard_001`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hard_a2c = model_path_prefix + \"exp_hard_001/tensor_obs/a2c/algo_default/seed_763.zip\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_agent_on_environment(hard_a2c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Slipping Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `exp_slip_1`\n",
    "\n",
    "_Environment Configuration:_\n",
    "\n",
    "```json\n",
    "    \"exp_slip_1\" : {\n",
    "        \"max_steps\" : 150,\n",
    "        \"slip_proba\" : 0.05,\n",
    "        \"wall_rebound\" : false,\n",
    "        \"spiky_active\" : false,\n",
    "        \"reward_spec\" : {\n",
    "            \"step_penalty\" : 0,\n",
    "            \"goal_reward\" : 1,\n",
    "            \"absorbing_states\" : false,\n",
    "            \"absorbing_reward_goal\" : 0,\n",
    "            \"absorbing_reward_lava\" : -1,\n",
    "            \"risky_tile_reward\" : 0,\n",
    "            \"lava_reward\" : -1\n",
    "        }\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `exp_slip_2`\n",
    "\n",
    "_Environment Configuration:_\n",
    "\n",
    "```json\n",
    "    \"exp_slip_2\" : {\n",
    "        \"max_steps\" : 150,\n",
    "        \"slip_proba\" : 0.1,\n",
    "        \"wall_rebound\" : false,\n",
    "        \"spiky_active\" : false,\n",
    "        \"reward_spec\" : {\n",
    "            \"step_penalty\" : 0,\n",
    "            \"goal_reward\" : 1,\n",
    "            \"absorbing_states\" : false,\n",
    "            \"absorbing_reward_goal\" : 0,\n",
    "            \"absorbing_reward_lava\" : -1,\n",
    "            \"risky_tile_reward\" : 0,\n",
    "            \"lava_reward\" : -1\n",
    "        }\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `exp_slip_3`\n",
    "\n",
    "_Environment Configuration:_\n",
    "\n",
    "```json\n",
    "    \"exp_slip_3\" : {\n",
    "        \"max_steps\" : 150,\n",
    "        \"slip_proba\" : 0.15,\n",
    "        \"wall_rebound\" : false,\n",
    "        \"spiky_active\" : false,\n",
    "        \"reward_spec\" : {\n",
    "            \"step_penalty\" : 0,\n",
    "            \"goal_reward\" : 1,\n",
    "            \"absorbing_states\" : false,\n",
    "            \"absorbing_reward_goal\" : 0,\n",
    "            \"absorbing_reward_lava\" : -1,\n",
    "            \"risky_tile_reward\" : 0,\n",
    "            \"lava_reward\" : -1\n",
    "        }\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO put this in time_penalty\n",
    "# TODO test how the agent reacts to changed environment (changed goal, changed lava etc.)\n",
    "\n",
    "tp_e = model_path_prefix + \"time_penalty/tensor_obs/dqn/algo_default/seed_763.zip\"\n",
    "test_agent_on_environment(tp_e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a,b,c = load_model_params(tp_e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## exp_hard_001\n",
    "\n",
    "_Environment Configuration:_\n",
    "\n",
    "```json\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 ('sb3')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0d41f5c1acf177a218bc0139b8f3c17ccec1007898cc8a9ab8dc8cb303ffab48"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
