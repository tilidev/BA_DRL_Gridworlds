{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating trained agents\n",
    "\n",
    "This Notebook will be used to visualize & analyze various trained agents on RiskyPath environment. Analysis will especially comprise observing the agent's behaviour in the environment it was trained for but also different versions of the environment (distributional shift analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "\n",
    "import gym\n",
    "import gym_minigrid\n",
    "from gym_minigrid.envs import RiskyPathEnv\n",
    "from gym_minigrid.wrappers import RGBImgObsWrapper, ImgObsWrapper, TensorObsWrapper\n",
    "\n",
    "from experiment_config import GridworldExperiment\n",
    "\n",
    "import stable_baselines3\n",
    "from stable_baselines3.dqn import DQN\n",
    "from stable_baselines3.a2c import A2C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilities\n",
    "\n",
    "Definition of functions to use for quick analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_agent_on_environment(\n",
    "    agent_path: str,\n",
    "    num_episodes: int = 1,\n",
    "    render_time: float = 0.3,\n",
    "    custom_environment: gym.Env = None,\n",
    "    predict_deterministic: bool = True,\n",
    "    accelerate_viz: bool = True\n",
    "):\n",
    "    # Extract model from path (a2c or dqn?)\n",
    "    if \"/dqn/\" in agent_path:\n",
    "        model_class = DQN\n",
    "    elif \"/a2c/\" in agent_path:\n",
    "        model_class = A2C\n",
    "\n",
    "    model = model_class.load(agent_path)\n",
    "    \n",
    "    # Create environment given information in function input\n",
    "    path_keys = agent_path.split(\"saved_models/\")[1].split(\"/\")\n",
    "    env_name = path_keys[0]\n",
    "    observation_type = path_keys[1]\n",
    "\n",
    "    render_size = 8\n",
    "    rgb = False\n",
    "    if \"pixel_obs_\" in agent_path:\n",
    "        render_size = int(path_keys[2].split(\"_\")[-1])\n",
    "        rgb = True\n",
    "\n",
    "    if custom_environment is None:\n",
    "        with open('env_config.json', 'r') as f:\n",
    "            env_kwargs = json.load(f)[env_name]\n",
    "            \n",
    "        env = gym.make(\n",
    "            \"MiniGrid-RiskyPath-v0\",\n",
    "            **env_kwargs\n",
    "        )\n",
    "    else:\n",
    "        env = custom_environment\n",
    "    \n",
    "    if rgb:\n",
    "        env = RGBImgObsWrapper(env, tile_size=render_size)\n",
    "        env = ImgObsWrapper(env)\n",
    "    else:\n",
    "        env = TensorObsWrapper(env)\n",
    "    \n",
    "    # Execute episodes and render agent\n",
    "        # TODO print reward, action [number, (himmelsrichtung)] etc.\n",
    "    for i in range(num_episodes):\n",
    "\n",
    "        print(f\"Starting episode {i+1}\")\n",
    "        total_reward = 0\n",
    "        needed_timesteps = 0\n",
    "\n",
    "        obs = env.reset()\n",
    "        done = False\n",
    "        env.render(tile_size=render_size)\n",
    "        time.sleep(render_time)\n",
    "\n",
    "        while not done:\n",
    "            action, _ = model.predict(obs, deterministic=predict_deterministic)\n",
    "            obs, reward, done, info = env.step(action)\n",
    "            env.render(tile_size=render_size)\n",
    "            total_reward += reward\n",
    "            needed_timesteps += 1\n",
    "            if needed_timesteps > 25:\n",
    "                render_time = 0.05\n",
    "            time.sleep(render_time)\n",
    "        \n",
    "        print(f\"Episode ended after {needed_timesteps} time steps.\")\n",
    "        out = f\"Total reward: {total_reward}\"\n",
    "        print(out)\n",
    "        print(\"-\"*len(out))\n",
    "    %matplotlib\n",
    "\n",
    "\n",
    "def make_env(\n",
    "    **kwargs\n",
    "):\n",
    "    env = gym.make(\n",
    "        \"MiniGrid-RiskyPath-v0\",\n",
    "        **kwargs\n",
    "    )\n",
    "    return env\n",
    "\n",
    "def load_model_params(\n",
    "    path: str\n",
    "):\n",
    "    # Extract model from path (a2c or dqn?)\n",
    "    if \"/dqn/\" in path:\n",
    "        model_class = DQN\n",
    "    elif \"/a2c/\" in path:\n",
    "        model_class = A2C\n",
    "\n",
    "    model = model_class.load(path)\n",
    "    return model.policy, model.policy_class, model.policy_kwargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ignore \"memory not enough\" warnings concerning replay buffer\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', module=\"stable_baselines3.common.buffers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE Save the prefix for the logs & models folder here for compatibility across different systems\n",
    "model_path_prefix = \"/Users/tilioschulze/Library/CloudStorage/OneDrive-Personal/Studium/Bachelorarbeit/experiment_models/saved_models/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib\n",
    "# Force matplotlib to render outside of notebook (Don't use 'inline' backend)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## exp_001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_001_path = model_path_prefix + \"exp_001/tensor_obs/dqn/algo_default/seed_763.zip\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_agent_on_environment(exp_001_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The agent solves the environment as expected.\n",
    "What would happen if the agent had to start from another position?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pos in [(7,6), (3,9), (9,7), (4,8)]:\n",
    "    test_agent_on_environment(\n",
    "        exp_001_path,\n",
    "        num_episodes=1,\n",
    "        custom_environment=make_env(\n",
    "            agent_start_pos=pos\n",
    "        ),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The agent successfully navigates the environment when beginning at another position. It quickly finds the goal tile and mostly doesn't take any detours.\n",
    "Interestingly, when the agent is placed on position (4,8) it first goes down and to the left instead of taking the quicker path upwards. Considering that the reward model of `exp_001` does not incentivize the agent to find the shortest path (no time penalty), this is not especially surprising. Still, this leads to the hypothesis that the agent found that going downwards from this position would lead to more reward than going up. (Or maybe due to the update rule in Q-Learning? --> # TODO investigate this)\n",
    "\n",
    "What happens when lava tiles are placed in the agent's way?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt_lava_positions = []\n",
    "for y in range(1, 11 - 1):\n",
    "    alt_lava_positions.append((1, y))\n",
    "for y in range(11 - 3, 11 - 8, -1):\n",
    "    alt_lava_positions.append((3, y))\n",
    "alt_lava_positions.extend([(6, 11 - 5), (6, 11 - 6)])\n",
    "alt_lava_positions.append((2,8))\n",
    "\n",
    "test_agent_on_environment(\n",
    "    exp_001_path,\n",
    "    num_episodes=1,\n",
    "    custom_environment=make_env(\n",
    "        lava_positions=alt_lava_positions\n",
    "    )\n",
    ")\n",
    "\n",
    "alt_lava_positions.append((3,3))\n",
    "test_agent_on_environment(\n",
    "    exp_001_path,\n",
    "    num_episodes=1,\n",
    "    custom_environment=make_env(\n",
    "        lava_positions=alt_lava_positions,\n",
    "        agent_start_pos=(4,7)\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that the agent is not able to recognize lava tiles. It always goes straight to the goal location and only evades the lava tile positions that it already knows from training. It seems the agent has not learned the causation between lava and negative reward but instead learned the correlation between the positions (of lava tiles) in the gridworld and the negative reward.\n",
    "\n",
    "This leads to the following question: \n",
    "Does the agent find the goal when it is placed somewhere else?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_agent_on_environment(\n",
    "    exp_001_path,\n",
    "    num_episodes=1,\n",
    "    custom_environment=make_env(\n",
    "        max_steps=25, # Changed limit because agent is caught in a loop\n",
    "        goal_positions=[(2,2)]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is clearly visible that the agent is not able to understand how to get to the goal tile when placed somewhere else. It gets caught in a loop and oscillated from left to right. Maybe an analysis would be interesting in which the q-net's output would be shown for each possible state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p, _, _ = load_model_params(exp_001_path)\n",
    "p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `time_penalty`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A2C on `time_penalty`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pen_a2c = model_path_prefix + \"time_penalty/tensor_obs/a2c/algo_default/seed_763.zip\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_agent_on_environment(\n",
    "    pen_a2c\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `exp_hard_001`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hard_a2c = model_path_prefix + \"exp_hard_001/tensor_obs/a2c/algo_default/seed_763.zip\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_agent_on_environment(hard_a2c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Slipping Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `exp_slip_1`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `exp_slip_2`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `exp_slip_3`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO put this in time_penalty\n",
    "# TODO test how the agent reacts to changed environment (changed goal, changed lava etc.)\n",
    "\n",
    "tp_e = model_path_prefix + \"time_penalty/tensor_obs/dqn/algo_default/seed_763.zip\"\n",
    "test_agent_on_environment(tp_e)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 ('sb3')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0d41f5c1acf177a218bc0139b8f3c17ccec1007898cc8a9ab8dc8cb303ffab48"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
